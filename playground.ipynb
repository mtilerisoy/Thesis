{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground Notebook For Quantizing VLP Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Distributed Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch.distributed as dist\n",
    "import copy\n",
    "\n",
    "# Limit the number of CPUs\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"  # Set this to the number of CPUs you want to use\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"  # Set this to the number of CPUs you want to use\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "# Initialize the process group\n",
    "dist.init_process_group(backend='gloo', init_method='env://', world_size=1, rank=0)\n",
    "\n",
    "# Verify initialization\n",
    "print(f\"Initialized: {dist.is_initialized()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"promote has been superseded by promote_options='default'\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Importing from timm.models.helpers is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Importing from timm.models.layers is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Importing from timm.models.registry is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_small_patch16_224 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_patch16_224 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_patch32_224 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_patch16_384 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_patch32_384 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_large_patch16_224 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_large_patch32_224 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_large_patch16_384 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_large_patch32_384 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_patch16_224_in21k in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_patch32_224_in21k in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_large_patch16_224_in21k in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_large_patch32_224_in21k in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_huge_patch14_224_in21k in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_resnet50_224_in21k in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_resnet50_384 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_small_resnet26d_224 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_resnet26d_224 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_resnet50d_224 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"You are using `torch.load` with `weights_only=False`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    \"\"\"\n",
    "    Function to print the size of the model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to get the size\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "\n",
    "def get_accuracy(pl_module, logits, target, device=\"cpu\"):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        logits, target = (\n",
    "            logits.detach().to(device),\n",
    "            target.detach().to(device),\n",
    "        )\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        preds = preds[target != -100]\n",
    "        target = target[target != -100]\n",
    "        if target.numel() == 0:\n",
    "            return 1\n",
    "\n",
    "        assert preds.shape == target.shape\n",
    "\n",
    "        correct += torch.sum(preds == target)\n",
    "        total += target.numel()\n",
    "\n",
    "        return correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Configuration to Initialize the Datamodule and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
      "  warnings.warn(\n",
      "WARNING:root:Found CUDA without GPU_NUM_DEVICES. Defaulting to PJRT_DEVICE=CUDA with GPU_NUM_DEVICES=2\n",
      "Seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import configs\n",
    "\n",
    "# Set the configuration\n",
    "_config = configs.vilt_config_vqav2\n",
    "_config[\"model_\"] = \"vilt\"\n",
    "_config[\"batch_size\"] = 32\n",
    "\n",
    "\n",
    "pl.seed_everything(_config[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Datamodule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a child datamodule that constructs a smaller version of the full datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-25 09:10:06.212024: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735114206.231243  836915 cuda_dnn.cc:8498] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735114206.237047  836915 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "from vilt.datamodules.multitask_datamodule import MTDataModule as MTDataModuleVILT\n",
    "from meter.datamodules.multitask_datamodule import MTDataModule as MTDataModuleMeter\n",
    "\n",
    "class SmallMTDataModuleVILT(MTDataModuleVILT):\n",
    "    def __init__(self, _config, dist=False, num_samples=5, start_idx=100):\n",
    "        super().__init__(_config, dist)\n",
    "        self.num_samples = num_samples\n",
    "        self.start_idx = start_idx\n",
    "\n",
    "    def setup(self, stage):\n",
    "        super().setup(stage)\n",
    "        \n",
    "        # Limit the number of samples in the datasets\n",
    "        self.train_dataset = Subset(self.train_dataset, range(self.start_idx, self.start_idx+self.num_samples))\n",
    "        self.val_dataset = Subset(self.val_dataset, range(self.start_idx, self.start_idx+self.num_samples))\n",
    "        self.test_dataset = Subset(self.test_dataset, range(self.start_idx, self.start_idx+self.num_samples))\n",
    "\n",
    "class SmallMTDataModuleMETER(MTDataModuleMeter):\n",
    "    def __init__(self, _config, dist=False, num_samples=10, start_idx=100):\n",
    "        super().__init__(_config, dist)\n",
    "        self.num_samples = num_samples\n",
    "        self.start_idx = start_idx\n",
    "\n",
    "    def setup(self, stage):\n",
    "        super().setup(stage)\n",
    "        \n",
    "        # Limit the number of samples in the datasets\n",
    "        self.train_dataset = Subset(self.train_dataset, range(self.start_idx, self.start_idx+self.num_samples))\n",
    "        self.val_dataset = Subset(self.val_dataset, range(self.start_idx, self.start_idx+self.num_samples))\n",
    "        self.test_dataset = Subset(self.test_dataset, range(self.start_idx, self.start_idx+self.num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the configuration and initialize the test and full datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded names: ['vqa_vlue_test']\n",
      "Loaded names: ['vqa_vlue_test']\n",
      "Loaded names: ['vqa_vlue_test']\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ========= Create full datamodule =========\n",
    "# ==========================================\n",
    "if \"meter\" in _config[\"model_\"]:\n",
    "    full_dm = MTDataModuleMeter(_config, dist=False)\n",
    "    \n",
    "    calibrarte_dm = SmallMTDataModuleMETER(_config, dist=False, num_samples=5, start_idx=100)\n",
    "    \n",
    "    infer_dm = SmallMTDataModuleMETER(_config, dist=False, num_samples=5, start_idx=0)\n",
    "    infer_dm.setup(\"test\")\n",
    "    infer_dataloader = infer_dm.test_dataloader()\n",
    "\n",
    "else:\n",
    "    full_dm = MTDataModuleVILT(_config, dist=False)\n",
    "\n",
    "    calibrarte_dm = SmallMTDataModuleVILT(_config, dist=False, num_samples=5, start_idx=100)\n",
    "    \n",
    "    infer_dm = SmallMTDataModuleVILT(_config, dist=False, num_samples=5, start_idx=0)\n",
    "    infer_dm.setup(\"test\")\n",
    "    infer_dataloader = infer_dm.test_dataloader()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized ViLT model\n"
     ]
    }
   ],
   "source": [
    "from vilt.modules import ViLTransformerSS\n",
    "from meter.modules import METERTransformerSS\n",
    "\n",
    "if _config[\"model_\"] == \"vilt\":\n",
    "    model = ViLTransformerSS(_config)\n",
    "    print(\"Initialized ViLT model\")\n",
    "\n",
    "elif _config[\"model_\"] == \"meter\":\n",
    "    model = METERTransformerSS(_config)\n",
    "    print(\"Initialized METER model\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Model not supported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize The Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded names: ['vqa_vlue_test']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded names: ['vqa_vlue_test']\n",
      "Loaded names: ['vqa_vlue_test']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2919e13ebcd644609f0ecc264d912d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('vqa/val/loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('vqa/val/score', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('vqa/val/score_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('vqa/val/loss_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val/the_metric', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      val/the_metric       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       vqa/val/loss        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    3.7881855964660645     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    vqa/val/loss_epoch     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    3.7881853580474854     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       vqa/val/score       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    vqa/val/score_epoch    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     val/the_metric      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      vqa/val/loss       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   3.7881855964660645    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   vqa/val/loss_epoch    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   3.7881853580474854    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      vqa/val/score      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   vqa/val/score_epoch   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'vqa/val/loss': 3.7881855964660645,\n",
       "  'vqa/val/score': 0.0,\n",
       "  'vqa/val/score_epoch': 0.0,\n",
       "  'vqa/val/loss_epoch': 3.7881853580474854,\n",
       "  'val/the_metric': 0.0}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========== Initialize the trainer for full precision ==========\n",
    "exp_name = f'{_config[\"exp_name\"]}'\n",
    "\n",
    "os.makedirs(_config[\"log_dir\"], exist_ok=True)\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val/the_metric\",\n",
    "    mode=\"max\",\n",
    "    save_last=True,\n",
    ")\n",
    "logger = pl.loggers.TensorBoardLogger(\n",
    "    _config[\"log_dir\"],\n",
    "    name=f'{exp_name}_seed{_config[\"seed\"]}_from_{_config[\"load_path\"].split(\"/\")[-1][:-5]}',\n",
    ")\n",
    "\n",
    "lr_callback = pl.callbacks.LearningRateMonitor(logging_interval=\"step\")\n",
    "callbacks = [checkpoint_callback, lr_callback]\n",
    "\n",
    "num_gpus = (\n",
    "    _config[\"num_gpus\"]\n",
    "    if isinstance(_config[\"num_gpus\"], int)\n",
    "    else len(_config[\"num_gpus\"])\n",
    ")\n",
    "\n",
    "grad_steps = _config[\"batch_size\"] // (\n",
    "    _config[\"per_gpu_batchsize\"] * num_gpus * _config[\"num_nodes\"]\n",
    ")\n",
    "\n",
    "max_steps = _config[\"max_steps\"] if _config[\"max_steps\"] is not None else None\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "        accelerator=\"cpu\",\n",
    "        devices=1,\n",
    "        num_nodes=_config[\"num_nodes\"],\n",
    "        precision=_config[\"precision\"],\n",
    "        # strategy=\"ddp\",\n",
    "        benchmark=True,\n",
    "        deterministic=False,\n",
    "        max_epochs=_config[\"max_epoch\"] if max_steps is None else 1000,\n",
    "        max_steps=max_steps,\n",
    "        callbacks=callbacks,\n",
    "        logger=logger,\n",
    "        # accumulate_grad_batches=grad_steps,\n",
    "        log_every_n_steps=10,\n",
    "        fast_dev_run=_config[\"fast_dev_run\"],\n",
    "        val_check_interval=_config[\"val_check_interval\"],\n",
    "    )\n",
    "\n",
    "trainer.test(model, datamodule=calibrarte_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization | PTQ to 8-bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after quantization:\n",
      "Size (MB): 125.73422\n"
     ]
    }
   ],
   "source": [
    "import dynamic_quantization as dq\n",
    "\n",
    "default_dynamic = copy.deepcopy(model)\n",
    "\n",
    "torch.quantization.quantize_dynamic(\n",
    "        default_dynamic, {torch.nn.Embedding}, dtype=torch.quint8, inplace=True\n",
    "    )\n",
    "\n",
    "torch.quantization.quantize_dynamic(\n",
    "        default_dynamic, {torch.nn.Linear, torch.nn.LayerNorm}, dtype=torch.qint8, inplace=True\n",
    "    )\n",
    "\n",
    "print(\"Size after quantization:\")\n",
    "print_size_of_model(default_dynamic)\n",
    "# print(model_dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Quantizing the model DYNAMIC =========\n",
      "Size after quantization:\n",
      "Size (MB): 125.73422\n"
     ]
    }
   ],
   "source": [
    "import dynamic_quantization as dq\n",
    "\n",
    "custom_8bit = copy.deepcopy(model)\n",
    "custom_8bit = dq.quantize_model_dynamic(custom_8bit, 8)\n",
    "\n",
    "print(\"Size after quantization:\")\n",
    "print_size_of_model(custom_8bit)\n",
    "# print(model_dynamic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numeric Suite Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization._numeric_suite as ns\n",
    "\n",
    "def compute_error(x, y):\n",
    "    \"\"\"\n",
    "    Signal to Noise Ratio (SNR)    \n",
    "    \"\"\"\n",
    "    Ps = torch.norm(x)\n",
    "    Pn = torch.norm(x-y)\n",
    "    return 20*torch.log10(Ps/Pn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - text_embeddings.LayerNorm.weight\n",
      "0 - inf\n",
      "1 - transformer.patch_embed.proj.weight\n",
      "1 - inf\n",
      "2 - transformer.blocks.0.norm1.weight\n",
      "2 - inf\n",
      "3 - transformer.blocks.0.attn.qkv._packed_params._packed_params\n",
      "3 - 27.72\n",
      "4 - transformer.blocks.0.attn.proj._packed_params._packed_params\n",
      "4 - 22.11\n",
      "5 - transformer.blocks.0.norm2.weight\n",
      "5 - inf\n",
      "6 - transformer.blocks.0.mlp.fc1._packed_params._packed_params\n",
      "6 - 26.74\n",
      "7 - transformer.blocks.0.mlp.fc2._packed_params._packed_params\n",
      "7 - 18.21\n",
      "8 - transformer.blocks.1.norm1.weight\n",
      "8 - inf\n",
      "9 - transformer.blocks.1.attn.qkv._packed_params._packed_params\n",
      "9 - 32.24\n",
      "10 - transformer.blocks.1.attn.proj._packed_params._packed_params\n",
      "10 - 29.18\n",
      "11 - transformer.blocks.1.norm2.weight\n",
      "11 - inf\n",
      "12 - transformer.blocks.1.mlp.fc1._packed_params._packed_params\n",
      "12 - 27.66\n",
      "13 - transformer.blocks.1.mlp.fc2._packed_params._packed_params\n",
      "13 - 16.82\n",
      "14 - transformer.blocks.2.norm1.weight\n",
      "14 - inf\n",
      "15 - transformer.blocks.2.attn.qkv._packed_params._packed_params\n",
      "15 - 34.03\n",
      "16 - transformer.blocks.2.attn.proj._packed_params._packed_params\n",
      "16 - 36.29\n",
      "17 - transformer.blocks.2.norm2.weight\n",
      "17 - inf\n",
      "18 - transformer.blocks.2.mlp.fc1._packed_params._packed_params\n",
      "18 - 32.70\n",
      "19 - transformer.blocks.2.mlp.fc2._packed_params._packed_params\n",
      "19 - 16.33\n",
      "20 - transformer.blocks.3.norm1.weight\n",
      "20 - inf\n",
      "21 - transformer.blocks.3.attn.qkv._packed_params._packed_params\n",
      "21 - 33.31\n",
      "22 - transformer.blocks.3.attn.proj._packed_params._packed_params\n",
      "22 - 37.09\n",
      "23 - transformer.blocks.3.norm2.weight\n",
      "23 - inf\n",
      "24 - transformer.blocks.3.mlp.fc1._packed_params._packed_params\n",
      "24 - 32.02\n",
      "25 - transformer.blocks.3.mlp.fc2._packed_params._packed_params\n",
      "25 - 30.65\n",
      "26 - transformer.blocks.4.norm1.weight\n",
      "26 - inf\n",
      "27 - transformer.blocks.4.attn.qkv._packed_params._packed_params\n",
      "27 - 34.66\n",
      "28 - transformer.blocks.4.attn.proj._packed_params._packed_params\n",
      "28 - 38.87\n",
      "29 - transformer.blocks.4.norm2.weight\n",
      "29 - inf\n",
      "30 - transformer.blocks.4.mlp.fc1._packed_params._packed_params\n",
      "30 - 31.60\n",
      "31 - transformer.blocks.4.mlp.fc2._packed_params._packed_params\n",
      "31 - 27.13\n",
      "32 - transformer.blocks.5.norm1.weight\n",
      "32 - inf\n",
      "33 - transformer.blocks.5.attn.qkv._packed_params._packed_params\n",
      "33 - 33.85\n",
      "34 - transformer.blocks.5.attn.proj._packed_params._packed_params\n",
      "34 - 38.24\n",
      "35 - transformer.blocks.5.norm2.weight\n",
      "35 - inf\n",
      "36 - transformer.blocks.5.mlp.fc1._packed_params._packed_params\n",
      "36 - 35.56\n",
      "37 - transformer.blocks.5.mlp.fc2._packed_params._packed_params\n",
      "37 - 29.89\n",
      "38 - transformer.blocks.6.norm1.weight\n",
      "38 - inf\n",
      "39 - transformer.blocks.6.attn.qkv._packed_params._packed_params\n",
      "39 - 35.75\n",
      "40 - transformer.blocks.6.attn.proj._packed_params._packed_params\n",
      "40 - 33.81\n",
      "41 - transformer.blocks.6.norm2.weight\n",
      "41 - inf\n",
      "42 - transformer.blocks.6.mlp.fc1._packed_params._packed_params\n",
      "42 - 31.80\n",
      "43 - transformer.blocks.6.mlp.fc2._packed_params._packed_params\n",
      "43 - 16.12\n",
      "44 - transformer.blocks.7.norm1.weight\n",
      "44 - inf\n",
      "45 - transformer.blocks.7.attn.qkv._packed_params._packed_params\n",
      "45 - 33.85\n",
      "46 - transformer.blocks.7.attn.proj._packed_params._packed_params\n",
      "46 - 37.72\n",
      "47 - transformer.blocks.7.norm2.weight\n",
      "47 - inf\n",
      "48 - transformer.blocks.7.mlp.fc1._packed_params._packed_params\n",
      "48 - 23.19\n",
      "49 - transformer.blocks.7.mlp.fc2._packed_params._packed_params\n",
      "49 - 15.13\n",
      "50 - transformer.blocks.8.norm1.weight\n",
      "50 - inf\n",
      "51 - transformer.blocks.8.attn.qkv._packed_params._packed_params\n",
      "51 - 35.31\n",
      "52 - transformer.blocks.8.attn.proj._packed_params._packed_params\n",
      "52 - 36.13\n",
      "53 - transformer.blocks.8.norm2.weight\n",
      "53 - inf\n",
      "54 - transformer.blocks.8.mlp.fc1._packed_params._packed_params\n",
      "54 - 18.87\n",
      "55 - transformer.blocks.8.mlp.fc2._packed_params._packed_params\n",
      "55 - 18.36\n",
      "56 - transformer.blocks.9.norm1.weight\n",
      "56 - inf\n",
      "57 - transformer.blocks.9.attn.qkv._packed_params._packed_params\n",
      "57 - 33.05\n",
      "58 - transformer.blocks.9.attn.proj._packed_params._packed_params\n",
      "58 - 30.61\n",
      "59 - transformer.blocks.9.norm2.weight\n",
      "59 - inf\n",
      "60 - transformer.blocks.9.mlp.fc1._packed_params._packed_params\n",
      "60 - 22.57\n",
      "61 - transformer.blocks.9.mlp.fc2._packed_params._packed_params\n",
      "61 - 21.04\n",
      "62 - transformer.blocks.10.norm1.weight\n",
      "62 - inf\n",
      "63 - transformer.blocks.10.attn.qkv._packed_params._packed_params\n",
      "63 - 30.86\n",
      "64 - transformer.blocks.10.attn.proj._packed_params._packed_params\n",
      "64 - 34.46\n",
      "65 - transformer.blocks.10.norm2.weight\n",
      "65 - inf\n",
      "66 - transformer.blocks.10.mlp.fc1._packed_params._packed_params\n",
      "66 - 33.35\n",
      "67 - transformer.blocks.10.mlp.fc2._packed_params._packed_params\n",
      "67 - 29.19\n",
      "68 - transformer.blocks.11.norm1.weight\n",
      "68 - inf\n",
      "69 - transformer.blocks.11.attn.qkv._packed_params._packed_params\n",
      "69 - 31.02\n",
      "70 - transformer.blocks.11.attn.proj._packed_params._packed_params\n",
      "70 - 26.65\n",
      "71 - transformer.blocks.11.norm2.weight\n",
      "71 - inf\n",
      "72 - transformer.blocks.11.mlp.fc1._packed_params._packed_params\n",
      "72 - 29.13\n",
      "73 - transformer.blocks.11.mlp.fc2._packed_params._packed_params\n",
      "73 - 19.53\n",
      "74 - transformer.norm.weight\n",
      "74 - inf\n",
      "75 - pooler.dense._packed_params._packed_params\n",
      "75 - 36.75\n",
      "76 - vqa_classifier.0._packed_params._packed_params\n",
      "76 - 37.66\n",
      "77 - vqa_classifier.1.weight\n",
      "77 - inf\n",
      "78 - vqa_classifier.3._packed_params._packed_params\n",
      "78 - 27.21\n",
      "Total error: 1502.01\n",
      "Total inf: 28\n",
      "Max error: 38.86937713623047\n"
     ]
    }
   ],
   "source": [
    "# ======== Dynamic quantization comparison ========\n",
    "wt_compare_dict_dynamic = ns.compare_weights(model.state_dict(), custom_8bit.state_dict())\n",
    "\n",
    "# print('keys of wt_compare_dict:')\n",
    "# print(wt_compare_dict_dynamic.keys())\n",
    "\n",
    "# key = 'text_embeddings.LayerNorm.weight'\n",
    "\n",
    "total_error = 0\n",
    "inf_count = 0\n",
    "max_err = 0\n",
    "for i, key in enumerate(wt_compare_dict_dynamic):\n",
    "    if wt_compare_dict_dynamic[key]['quantized'].is_quantized:\n",
    "        err = compute_error(wt_compare_dict_dynamic[key]['float'], wt_compare_dict_dynamic[key]['quantized'].dequantize())\n",
    "        \n",
    "        print(f\"{i} - {key}\")\n",
    "        print(f\"{i} - {err:.2f}\")\n",
    "        \n",
    "        if not torch.isinf(err):\n",
    "            total_error += err\n",
    "            if err > max_err:\n",
    "                max_err = err\n",
    "        else:\n",
    "            inf_count += 1\n",
    "    else:\n",
    "        err = compute_error(wt_compare_dict_dynamic[key]['float'], wt_compare_dict_dynamic[key]['quantized'])\n",
    "        \n",
    "        print(f\"{i} - {key}\")\n",
    "        print(f\"{i} - {err:.2f}\")\n",
    "\n",
    "        if not torch.isinf(err):\n",
    "            total_error += err\n",
    "            if err > max_err:\n",
    "                max_err = err\n",
    "        else:\n",
    "            inf_count += 1\n",
    "\n",
    "print(f\"Total error: {total_error:.2f}\")\n",
    "print(f\"Total inf: {inf_count}\")\n",
    "print(f\"Max error: {max_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_batch = next(iter(infer_dataloader))\n",
    "calibration_batch = next(iter(calibrarte_dm.val_dataloader()))\n",
    "# full_batch = next(iter(full_dm.val_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/pytorch_lightning/core/module.py:445: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text_embeddings.LayerNorm.stats', 'text_embeddings.quant.stats', 'transformer.patch_embed.proj.stats', 'transformer.blocks.0.norm1.stats', 'transformer.blocks.0.attn.qkv.stats', 'transformer.blocks.0.attn.proj.stats', 'transformer.blocks.0.norm2.stats', 'transformer.blocks.0.mlp.fc1.stats', 'transformer.blocks.0.mlp.fc2.stats', 'transformer.blocks.1.norm1.stats', 'transformer.blocks.1.attn.qkv.stats', 'transformer.blocks.1.attn.proj.stats', 'transformer.blocks.1.norm2.stats', 'transformer.blocks.1.mlp.fc1.stats', 'transformer.blocks.1.mlp.fc2.stats', 'transformer.blocks.2.norm1.stats', 'transformer.blocks.2.attn.qkv.stats', 'transformer.blocks.2.attn.proj.stats', 'transformer.blocks.2.norm2.stats', 'transformer.blocks.2.mlp.fc1.stats', 'transformer.blocks.2.mlp.fc2.stats', 'transformer.blocks.3.norm1.stats', 'transformer.blocks.3.attn.qkv.stats', 'transformer.blocks.3.attn.proj.stats', 'transformer.blocks.3.norm2.stats', 'transformer.blocks.3.mlp.fc1.stats', 'transformer.blocks.3.mlp.fc2.stats', 'transformer.blocks.4.norm1.stats', 'transformer.blocks.4.attn.qkv.stats', 'transformer.blocks.4.attn.proj.stats', 'transformer.blocks.4.norm2.stats', 'transformer.blocks.4.mlp.fc1.stats', 'transformer.blocks.4.mlp.fc2.stats', 'transformer.blocks.5.norm1.stats', 'transformer.blocks.5.attn.qkv.stats', 'transformer.blocks.5.attn.proj.stats', 'transformer.blocks.5.norm2.stats', 'transformer.blocks.5.mlp.fc1.stats', 'transformer.blocks.5.mlp.fc2.stats', 'transformer.blocks.6.norm1.stats', 'transformer.blocks.6.attn.qkv.stats', 'transformer.blocks.6.attn.proj.stats', 'transformer.blocks.6.norm2.stats', 'transformer.blocks.6.mlp.fc1.stats', 'transformer.blocks.6.mlp.fc2.stats', 'transformer.blocks.7.norm1.stats', 'transformer.blocks.7.attn.qkv.stats', 'transformer.blocks.7.attn.proj.stats', 'transformer.blocks.7.norm2.stats', 'transformer.blocks.7.mlp.fc1.stats', 'transformer.blocks.7.mlp.fc2.stats', 'transformer.blocks.8.norm1.stats', 'transformer.blocks.8.attn.qkv.stats', 'transformer.blocks.8.attn.proj.stats', 'transformer.blocks.8.norm2.stats', 'transformer.blocks.8.mlp.fc1.stats', 'transformer.blocks.8.mlp.fc2.stats', 'transformer.blocks.9.norm1.stats', 'transformer.blocks.9.attn.qkv.stats', 'transformer.blocks.9.attn.proj.stats', 'transformer.blocks.9.norm2.stats', 'transformer.blocks.9.mlp.fc1.stats', 'transformer.blocks.9.mlp.fc2.stats', 'transformer.blocks.10.norm1.stats', 'transformer.blocks.10.attn.qkv.stats', 'transformer.blocks.10.attn.proj.stats', 'transformer.blocks.10.norm2.stats', 'transformer.blocks.10.mlp.fc1.stats', 'transformer.blocks.10.mlp.fc2.stats', 'transformer.blocks.11.norm1.stats', 'transformer.blocks.11.attn.qkv.stats', 'transformer.blocks.11.attn.proj.stats', 'transformer.blocks.11.norm2.stats', 'transformer.blocks.11.mlp.fc1.stats', 'transformer.blocks.11.mlp.fc2.stats', 'transformer.norm.stats', 'pooler.dense.stats', 'vqa_classifier.0.stats', 'vqa_classifier.1.stats', 'vqa_classifier.3.stats'])\n"
     ]
    }
   ],
   "source": [
    "# act_compare_dict_dynamic = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(model_dynamic), full_batch)\n",
    "act_compare_dict_dynamic = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(custom_8bit), infer_batch)\n",
    "print(act_compare_dict_dynamic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - text_embeddings.LayerNorm.stats\n",
      "0 - 39.83\n",
      "1 - text_embeddings.quant.stats\n",
      "1 - 39.09\n",
      "2 - transformer.patch_embed.proj.stats\n",
      "2 - inf\n",
      "3 - transformer.blocks.0.norm1.stats\n",
      "3 - 9.85\n",
      "4 - transformer.blocks.0.attn.qkv.stats\n",
      "4 - 21.50\n",
      "5 - transformer.blocks.0.attn.proj.stats\n",
      "5 - 11.46\n",
      "6 - transformer.blocks.0.norm2.stats\n",
      "6 - 8.84\n",
      "7 - transformer.blocks.0.mlp.fc1.stats\n",
      "7 - 18.13\n",
      "8 - transformer.blocks.0.mlp.fc2.stats\n",
      "8 - 9.53\n",
      "9 - transformer.blocks.1.norm1.stats\n",
      "9 - 7.37\n",
      "10 - transformer.blocks.1.attn.qkv.stats\n",
      "10 - 17.58\n",
      "11 - transformer.blocks.1.attn.proj.stats\n",
      "11 - 7.76\n",
      "12 - transformer.blocks.1.norm2.stats\n",
      "12 - 8.30\n",
      "13 - transformer.blocks.1.mlp.fc1.stats\n",
      "13 - 14.61\n",
      "14 - transformer.blocks.1.mlp.fc2.stats\n",
      "14 - 4.42\n",
      "15 - transformer.blocks.2.norm1.stats\n",
      "15 - 6.03\n",
      "16 - transformer.blocks.2.attn.qkv.stats\n",
      "16 - 12.27\n",
      "17 - transformer.blocks.2.attn.proj.stats\n",
      "17 - 8.19\n",
      "18 - transformer.blocks.2.norm2.stats\n",
      "18 - 7.17\n",
      "19 - transformer.blocks.2.mlp.fc1.stats\n",
      "19 - 13.23\n",
      "20 - transformer.blocks.2.mlp.fc2.stats\n",
      "20 - 3.44\n",
      "21 - transformer.blocks.3.norm1.stats\n",
      "21 - 5.09\n",
      "22 - transformer.blocks.3.attn.qkv.stats\n",
      "22 - 10.27\n",
      "23 - transformer.blocks.3.attn.proj.stats\n",
      "23 - 8.35\n",
      "24 - transformer.blocks.3.norm2.stats\n",
      "24 - 6.54\n",
      "25 - transformer.blocks.3.mlp.fc1.stats\n",
      "25 - 12.67\n",
      "26 - transformer.blocks.3.mlp.fc2.stats\n",
      "26 - 3.35\n",
      "27 - transformer.blocks.4.norm1.stats\n",
      "27 - 4.90\n",
      "28 - transformer.blocks.4.attn.qkv.stats\n",
      "28 - 10.36\n",
      "29 - transformer.blocks.4.attn.proj.stats\n",
      "29 - 7.91\n",
      "30 - transformer.blocks.4.norm2.stats\n",
      "30 - 6.16\n",
      "31 - transformer.blocks.4.mlp.fc1.stats\n",
      "31 - 12.67\n",
      "32 - transformer.blocks.4.mlp.fc2.stats\n",
      "32 - 3.02\n",
      "33 - transformer.blocks.5.norm1.stats\n",
      "33 - 4.66\n",
      "34 - transformer.blocks.5.attn.qkv.stats\n",
      "34 - 10.30\n",
      "35 - transformer.blocks.5.attn.proj.stats\n",
      "35 - 7.75\n",
      "36 - transformer.blocks.5.norm2.stats\n",
      "36 - 5.87\n",
      "37 - transformer.blocks.5.mlp.fc1.stats\n",
      "37 - 12.53\n",
      "38 - transformer.blocks.5.mlp.fc2.stats\n",
      "38 - 2.64\n",
      "39 - transformer.blocks.6.norm1.stats\n",
      "39 - 4.46\n",
      "40 - transformer.blocks.6.attn.qkv.stats\n",
      "40 - 10.63\n",
      "41 - transformer.blocks.6.attn.proj.stats\n",
      "41 - 6.24\n",
      "42 - transformer.blocks.6.norm2.stats\n",
      "42 - 5.40\n",
      "43 - transformer.blocks.6.mlp.fc1.stats\n",
      "43 - 11.52\n",
      "44 - transformer.blocks.6.mlp.fc2.stats\n",
      "44 - -0.42\n",
      "45 - transformer.blocks.7.norm1.stats\n",
      "45 - 4.04\n",
      "46 - transformer.blocks.7.attn.qkv.stats\n",
      "46 - 13.34\n",
      "47 - transformer.blocks.7.attn.proj.stats\n",
      "47 - 6.39\n",
      "48 - transformer.blocks.7.norm2.stats\n",
      "48 - 4.73\n",
      "49 - transformer.blocks.7.mlp.fc1.stats\n",
      "49 - 10.09\n",
      "50 - transformer.blocks.7.mlp.fc2.stats\n",
      "50 - 1.36\n",
      "51 - transformer.blocks.8.norm1.stats\n",
      "51 - 3.65\n",
      "52 - transformer.blocks.8.attn.qkv.stats\n",
      "52 - 13.88\n",
      "53 - transformer.blocks.8.attn.proj.stats\n",
      "53 - 3.48\n",
      "54 - transformer.blocks.8.norm2.stats\n",
      "54 - 3.82\n",
      "55 - transformer.blocks.8.mlp.fc1.stats\n",
      "55 - 8.74\n",
      "56 - transformer.blocks.8.mlp.fc2.stats\n",
      "56 - 0.68\n",
      "57 - transformer.blocks.9.norm1.stats\n",
      "57 - 2.93\n",
      "58 - transformer.blocks.9.attn.qkv.stats\n",
      "58 - 14.09\n",
      "59 - transformer.blocks.9.attn.proj.stats\n",
      "59 - 3.08\n",
      "60 - transformer.blocks.9.norm2.stats\n",
      "60 - 2.88\n",
      "61 - transformer.blocks.9.mlp.fc1.stats\n",
      "61 - 7.91\n",
      "62 - transformer.blocks.9.mlp.fc2.stats\n",
      "62 - 1.73\n",
      "63 - transformer.blocks.10.norm1.stats\n",
      "63 - 1.89\n",
      "64 - transformer.blocks.10.attn.qkv.stats\n",
      "64 - 12.69\n",
      "65 - transformer.blocks.10.attn.proj.stats\n",
      "65 - 1.97\n",
      "66 - transformer.blocks.10.norm2.stats\n",
      "66 - 2.39\n",
      "67 - transformer.blocks.10.mlp.fc1.stats\n",
      "67 - 9.92\n",
      "68 - transformer.blocks.10.mlp.fc2.stats\n",
      "68 - 0.92\n",
      "69 - transformer.blocks.11.norm1.stats\n",
      "69 - 1.26\n",
      "70 - transformer.blocks.11.attn.qkv.stats\n",
      "70 - 11.49\n",
      "71 - transformer.blocks.11.attn.proj.stats\n",
      "71 - 5.75\n",
      "72 - transformer.blocks.11.norm2.stats\n",
      "72 - 1.39\n",
      "73 - transformer.blocks.11.mlp.fc1.stats\n",
      "73 - 7.73\n",
      "74 - transformer.blocks.11.mlp.fc2.stats\n",
      "74 - 1.59\n",
      "75 - transformer.norm.stats\n",
      "75 - 4.56\n",
      "76 - pooler.dense.stats\n",
      "76 - 1.62\n",
      "77 - vqa_classifier.0.stats\n",
      "77 - 5.03\n",
      "78 - vqa_classifier.1.stats\n",
      "78 - 10.43\n",
      "79 - vqa_classifier.3.stats\n",
      "79 - 25.33\n",
      "Total error: 654.26\n"
     ]
    }
   ],
   "source": [
    "total_err = 0\n",
    "inf_count = 0\n",
    "max_err = 0\n",
    "for idx, key in enumerate(act_compare_dict_dynamic):\n",
    "    err = compute_error(act_compare_dict_dynamic[key]['float'][0][0], act_compare_dict_dynamic[key]['quantized'][0][0])\n",
    "    # print(type(err))\n",
    "    if torch.isinf(err):\n",
    "        inf_count += 1\n",
    "    else:\n",
    "        total_err += err\n",
    "        if err > max_err:\n",
    "            max_err = err\n",
    "    print(f\"{idx} - {key}\")\n",
    "    print(f\"{idx} - {err:.2f}\")\n",
    "\n",
    "print(f\"Total error: {total_err:.2f}\")\n",
    "# print(f\"Total inf: {inf_count}\")\n",
    "# print(f\"Max error: {max_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViLTransformerSS(\n",
      "  (text_embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(40, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (quant): QuantStub()\n",
      "    (dequant): DeQuantStub()\n",
      "  )\n",
      "  (token_type_embeddings): Embedding(3, 768)\n",
      "  (transformer): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      "      (quant): QuantStub()\n",
      "      (dequant): DeQuantStub()\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.1, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "          (quant): QuantStub()\n",
      "          (dequant): DeQuantStub()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): QuantStub()\n",
      "        (dequant): DeQuantStub()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (pooler): Pooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "    (quant): QuantStub()\n",
      "    (dequant): DeQuantStub()\n",
      "  )\n",
      "  (nlvr2_classifier): Sequential(\n",
      "    (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): Linear(in_features=1536, out_features=2, bias=True)\n",
      "  )\n",
      "  (train_nlvr2_accuracy): Accuracy()\n",
      "  (train_nlvr2_loss): Scalar()\n",
      "  (dev_nlvr2_accuracy): Accuracy()\n",
      "  (dev_nlvr2_loss): Scalar()\n",
      "  (test_nlvr2_accuracy): Accuracy()\n",
      "  (test_nlvr2_loss): Scalar()\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViLTransformerSS(\n",
      "  (text_embeddings): BertEmbeddings(\n",
      "    (word_embeddings): QuantizedEmbedding(num_embeddings=30522, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
      "    (position_embeddings): QuantizedEmbedding(num_embeddings=40, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
      "    (token_type_embeddings): QuantizedEmbedding(num_embeddings=2, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
      "    (LayerNorm): QuantizedLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): QuantizedDropout(p=0.1, inplace=False)\n",
      "    (quant): Quantize(scale=tensor([0.0120]), zero_point=tensor([63]), dtype=torch.quint8)\n",
      "    (dequant): DeQuantize()\n",
      "  )\n",
      "  (token_type_embeddings): QuantizedEmbedding(num_embeddings=3, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
      "  (transformer): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): QuantizedConv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), scale=0.2465784102678299, zero_point=61)\n",
      "      (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
      "      (dequant): DeQuantize()\n",
      "    )\n",
      "    (pos_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): QuantizedLinear(in_features=768, out_features=2304, scale=0.12492784112691879, zero_point=64, qscheme=torch.per_channel_affine)\n",
      "          (attn_drop): QuantizedDropout(p=0.0, inplace=False)\n",
      "          (proj): QuantizedLinear(in_features=768, out_features=768, scale=0.27863624691963196, zero_point=100, qscheme=torch.per_channel_affine)\n",
      "          (proj_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "          (quant): Quantize(scale=tensor([0.0385]), zero_point=tensor([62]), dtype=torch.quint8)\n",
      "          (dequant): DeQuantize()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.10200261324644089, zero_point=79, qscheme=torch.per_channel_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=0.5394390225410461, zero_point=106, qscheme=torch.per_channel_affine)\n",
      "          (drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): Quantize(scale=tensor([0.2708]), zero_point=tensor([61]), dtype=torch.quint8)\n",
      "        (dequant): DeQuantize()\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): QuantizedLinear(in_features=768, out_features=2304, scale=0.15472693741321564, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "          (attn_drop): QuantizedDropout(p=0.0, inplace=False)\n",
      "          (proj): QuantizedLinear(in_features=768, out_features=768, scale=0.0534946471452713, zero_point=65, qscheme=torch.per_channel_affine)\n",
      "          (proj_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "          (quant): Quantize(scale=tensor([0.0372]), zero_point=tensor([61]), dtype=torch.quint8)\n",
      "          (dequant): DeQuantize()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.09735607355833054, zero_point=96, qscheme=torch.per_channel_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=0.23655180633068085, zero_point=106, qscheme=torch.per_channel_affine)\n",
      "          (drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): Quantize(scale=tensor([0.8231]), zero_point=tensor([105]), dtype=torch.quint8)\n",
      "        (dequant): DeQuantize()\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): QuantizedLinear(in_features=768, out_features=2304, scale=0.10447195917367935, zero_point=64, qscheme=torch.per_channel_affine)\n",
      "          (attn_drop): QuantizedDropout(p=0.0, inplace=False)\n",
      "          (proj): QuantizedLinear(in_features=768, out_features=768, scale=0.05652575567364693, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "          (proj_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "          (quant): Quantize(scale=tensor([0.0421]), zero_point=tensor([63]), dtype=torch.quint8)\n",
      "          (dequant): DeQuantize()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.08510777354240417, zero_point=90, qscheme=torch.per_channel_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=0.19182977080345154, zero_point=98, qscheme=torch.per_channel_affine)\n",
      "          (drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): Quantize(scale=tensor([1.0681]), zero_point=tensor([105]), dtype=torch.quint8)\n",
      "        (dequant): DeQuantize()\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): QuantizedLinear(in_features=768, out_features=2304, scale=0.12167397886514664, zero_point=61, qscheme=torch.per_channel_affine)\n",
      "          (attn_drop): QuantizedDropout(p=0.0, inplace=False)\n",
      "          (proj): QuantizedLinear(in_features=768, out_features=768, scale=0.058553487062454224, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "          (proj_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "          (quant): Quantize(scale=tensor([0.0390]), zero_point=tensor([63]), dtype=torch.quint8)\n",
      "          (dequant): DeQuantize()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.07369934767484665, zero_point=86, qscheme=torch.per_channel_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=0.06079676002264023, zero_point=66, qscheme=torch.per_channel_affine)\n",
      "          (drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): Quantize(scale=tensor([1.2680]), zero_point=tensor([103]), dtype=torch.quint8)\n",
      "        (dequant): DeQuantize()\n",
      "      )\n",
      "      (4): Block(\n",
      "        (norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): QuantizedLinear(in_features=768, out_features=2304, scale=0.11904111504554749, zero_point=65, qscheme=torch.per_channel_affine)\n",
      "          (attn_drop): QuantizedDropout(p=0.0, inplace=False)\n",
      "          (proj): QuantizedLinear(in_features=768, out_features=768, scale=0.05614360421895981, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "          (proj_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "          (quant): Quantize(scale=tensor([0.0378]), zero_point=tensor([63]), dtype=torch.quint8)\n",
      "          (dequant): DeQuantize()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.07786823064088821, zero_point=84, qscheme=torch.per_channel_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=0.12002807855606079, zero_point=77, qscheme=torch.per_channel_affine)\n",
      "          (drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): Quantize(scale=tensor([1.2903]), zero_point=tensor([102]), dtype=torch.quint8)\n",
      "        (dequant): DeQuantize()\n",
      "      )\n",
      "      (5): Block(\n",
      "        (norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): QuantizedLinear(in_features=768, out_features=2304, scale=0.1375829130411148, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "          (attn_drop): QuantizedDropout(p=0.0, inplace=False)\n",
      "          (proj): QuantizedLinear(in_features=768, out_features=768, scale=0.07467816770076752, zero_point=59, qscheme=torch.per_channel_affine)\n",
      "          (proj_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "          (quant): Quantize(scale=tensor([0.0427]), zero_point=tensor([61]), dtype=torch.quint8)\n",
      "          (dequant): DeQuantize()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.09074079245328903, zero_point=85, qscheme=torch.per_channel_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=0.18053790926933289, zero_point=75, qscheme=torch.per_channel_affine)\n",
      "          (drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): Quantize(scale=tensor([1.3396]), zero_point=tensor([100]), dtype=torch.quint8)\n",
      "        (dequant): DeQuantize()\n",
      "      )\n",
      "      (6): Block(\n",
      "        (norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): QuantizedLinear(in_features=768, out_features=2304, scale=0.15529251098632812, zero_point=64, qscheme=torch.per_channel_affine)\n",
      "          (attn_drop): QuantizedDropout(p=0.0, inplace=False)\n",
      "          (proj): QuantizedLinear(in_features=768, out_features=768, scale=0.13922618329524994, zero_point=58, qscheme=torch.per_channel_affine)\n",
      "          (proj_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "          (quant): Quantize(scale=tensor([0.0456]), zero_point=tensor([61]), dtype=torch.quint8)\n",
      "          (dequant): DeQuantize()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.22372540831565857, zero_point=72, qscheme=torch.per_channel_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=5.199750900268555, zero_point=80, qscheme=torch.per_channel_affine)\n",
      "          (drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): Quantize(scale=tensor([1.3721]), zero_point=tensor([99]), dtype=torch.quint8)\n",
      "        (dequant): DeQuantize()\n",
      "      )\n",
      "      (7): Block(\n",
      "        (norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): QuantizedLinear(in_features=768, out_features=2304, scale=0.1954934298992157, zero_point=67, qscheme=torch.per_channel_affine)\n",
      "          (attn_drop): QuantizedDropout(p=0.0, inplace=False)\n",
      "          (proj): QuantizedLinear(in_features=768, out_features=768, scale=0.23527701199054718, zero_point=65, qscheme=torch.per_channel_affine)\n",
      "          (proj_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "          (quant): Quantize(scale=tensor([0.0554]), zero_point=tensor([66]), dtype=torch.quint8)\n",
      "          (dequant): DeQuantize()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.651413083076477, zero_point=74, qscheme=torch.per_channel_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=12.620697021484375, zero_point=78, qscheme=torch.per_channel_affine)\n",
      "          (drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): Quantize(scale=tensor([5.1881]), zero_point=tensor([83]), dtype=torch.quint8)\n",
      "        (dequant): DeQuantize()\n",
      "      )\n",
      "      (8): Block(\n",
      "        (norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): QuantizedLinear(in_features=768, out_features=2304, scale=0.22603781521320343, zero_point=62, qscheme=torch.per_channel_affine)\n",
      "          (attn_drop): QuantizedDropout(p=0.0, inplace=False)\n",
      "          (proj): QuantizedLinear(in_features=768, out_features=768, scale=0.1469428390264511, zero_point=61, qscheme=torch.per_channel_affine)\n",
      "          (proj_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "          (quant): Quantize(scale=tensor([0.0527]), zero_point=tensor([62]), dtype=torch.quint8)\n",
      "          (dequant): DeQuantize()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.6497867107391357, zero_point=82, qscheme=torch.per_channel_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=8.821465492248535, zero_point=81, qscheme=torch.per_channel_affine)\n",
      "          (drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): Quantize(scale=tensor([15.2433]), zero_point=tensor([79]), dtype=torch.quint8)\n",
      "        (dequant): DeQuantize()\n",
      "      )\n",
      "      (9): Block(\n",
      "        (norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): QuantizedLinear(in_features=768, out_features=2304, scale=0.2295437902212143, zero_point=62, qscheme=torch.per_channel_affine)\n",
      "          (attn_drop): QuantizedDropout(p=0.0, inplace=False)\n",
      "          (proj): QuantizedLinear(in_features=768, out_features=768, scale=0.2954007387161255, zero_point=62, qscheme=torch.per_channel_affine)\n",
      "          (proj_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "          (quant): Quantize(scale=tensor([0.0707]), zero_point=tensor([65]), dtype=torch.quint8)\n",
      "          (dequant): DeQuantize()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.3547573983669281, zero_point=81, qscheme=torch.per_channel_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=4.396595478057861, zero_point=67, qscheme=torch.per_channel_affine)\n",
      "          (drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): Quantize(scale=tensor([27.0621]), zero_point=tensor([69]), dtype=torch.quint8)\n",
      "        (dequant): DeQuantize()\n",
      "      )\n",
      "      (10): Block(\n",
      "        (norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): QuantizedLinear(in_features=768, out_features=2304, scale=0.28237566351890564, zero_point=62, qscheme=torch.per_channel_affine)\n",
      "          (attn_drop): QuantizedDropout(p=0.0, inplace=False)\n",
      "          (proj): QuantizedLinear(in_features=768, out_features=768, scale=0.32072168588638306, zero_point=72, qscheme=torch.per_channel_affine)\n",
      "          (proj_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "          (quant): Quantize(scale=tensor([0.0888]), zero_point=tensor([59]), dtype=torch.quint8)\n",
      "          (dequant): DeQuantize()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.4795296788215637, zero_point=89, qscheme=torch.per_channel_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=1.0709540843963623, zero_point=53, qscheme=torch.per_channel_affine)\n",
      "          (drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): Quantize(scale=tensor([31.4832]), zero_point=tensor([68]), dtype=torch.quint8)\n",
      "        (dequant): DeQuantize()\n",
      "      )\n",
      "      (11): Block(\n",
      "        (norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): QuantizedLinear(in_features=768, out_features=2304, scale=0.25789180397987366, zero_point=67, qscheme=torch.per_channel_affine)\n",
      "          (attn_drop): QuantizedDropout(p=0.0, inplace=False)\n",
      "          (proj): QuantizedLinear(in_features=768, out_features=768, scale=4.655595302581787, zero_point=83, qscheme=torch.per_channel_affine)\n",
      "          (proj_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "          (quant): Quantize(scale=tensor([0.1539]), zero_point=tensor([60]), dtype=torch.quint8)\n",
      "          (dequant): DeQuantize()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.6355603337287903, zero_point=50, qscheme=torch.per_channel_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=14.024627685546875, zero_point=82, qscheme=torch.per_channel_affine)\n",
      "          (drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): Quantize(scale=tensor([32.5525]), zero_point=tensor([66]), dtype=torch.quint8)\n",
      "        (dequant): DeQuantize()\n",
      "      )\n",
      "    )\n",
      "    (norm): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (pooler): Pooler(\n",
      "    (dense): QuantizedLinear(in_features=768, out_features=768, scale=0.07670333236455917, zero_point=61, qscheme=torch.per_channel_affine)\n",
      "    (activation): Tanh()\n",
      "    (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
      "    (dequant): DeQuantize()\n",
      "  )\n",
      "  (nlvr2_classifier): Sequential(\n",
      "    (0): QuantizedLinear(in_features=1536, out_features=1536, scale=0.07336397469043732, zero_point=52, qscheme=torch.per_channel_affine)\n",
      "    (1): QuantizedLayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): QuantizedLinear(in_features=1536, out_features=2, scale=0.069314144551754, zero_point=53, qscheme=torch.per_channel_affine)\n",
      "  )\n",
      "  (train_nlvr2_accuracy): Accuracy()\n",
      "  (train_nlvr2_loss): Scalar()\n",
      "  (dev_nlvr2_accuracy): Accuracy()\n",
      "  (dev_nlvr2_loss): Scalar()\n",
      "  (test_nlvr2_accuracy): Accuracy()\n",
      "  (test_nlvr2_loss): Scalar()\n",
      "  (quant): Quantize(scale=tensor([32.1737]), zero_point=tensor([66]), dtype=torch.quint8)\n",
      "  (dequant): DeQuantize()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_static)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
