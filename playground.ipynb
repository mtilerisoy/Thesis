{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground Notebook For Quantizing VLP Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Distributed Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch.distributed as dist\n",
    "import copy\n",
    "\n",
    "# Limit the number of CPUs\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"  # Set this to the number of CPUs you want to use\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"  # Set this to the number of CPUs you want to use\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "# Initialize the process group\n",
    "dist.init_process_group(backend='gloo', init_method='env://', world_size=1, rank=0)\n",
    "\n",
    "# Verify initialization\n",
    "print(f\"Initialized: {dist.is_initialized()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"promote has been superseded by promote_options='default'\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Importing from timm.models.helpers is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Importing from timm.models.layers is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Importing from timm.models.registry is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_small_patch16_224 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_patch16_224 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_patch32_224 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_patch16_384 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_patch32_384 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_large_patch16_224 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_large_patch32_224 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_large_patch16_384 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_large_patch32_384 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_patch16_224_in21k in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_patch32_224_in21k in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_large_patch16_224_in21k in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_large_patch32_224_in21k in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_huge_patch14_224_in21k in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_resnet50_224_in21k in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_resnet50_384 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_small_resnet26d_224 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_resnet26d_224 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_resnet50d_224 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"You are using `torch.load` with `weights_only=False`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    \"\"\"\n",
    "    Function to print the size of the model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to get the size\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "\n",
    "def get_accuracy(pl_module, logits, target, device=\"cpu\"):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        logits, target = (\n",
    "            logits.detach().to(device),\n",
    "            target.detach().to(device),\n",
    "        )\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        preds = preds[target != -100]\n",
    "        target = target[target != -100]\n",
    "        if target.numel() == 0:\n",
    "            return 1\n",
    "\n",
    "        assert preds.shape == target.shape\n",
    "\n",
    "        correct += torch.sum(preds == target)\n",
    "        total += target.numel()\n",
    "\n",
    "        return correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Configuration to Initialize the Datamodule and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
      "  warnings.warn(\n",
      "WARNING:root:Found CUDA without GPU_NUM_DEVICES. Defaulting to PJRT_DEVICE=CUDA with GPU_NUM_DEVICES=2\n",
      "Seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "# Define the configuration for the experiments\n",
    "vilt_config_nlvr2 = {'exp_name': 'test_ood_nlvr2', 'seed': 0, 'datasets': ['ood_nlvr2'], 'loss_names': {'itm': 0, 'mlm': 0, 'mpp': 0, 'vqa': 0, 'nlvr2': 1, 'irtr': 0}, 'batch_size': 128, 'accelerator': 'gpu', 'train_transform_keys': ['pixelbert_randaug'], 'val_transform_keys': ['pixelbert'], 'image_size': 384, 'max_image_len': -1, 'patch_size': 32, 'draw_false_image': 0, 'image_only': False, 'vqav2_label_size': 3129, 'max_text_len': 40, 'tokenizer': 'bert-base-uncased', 'vocab_size': 30522, 'whole_word_masking': False, 'mlm_prob': 0.15, 'draw_false_text': 0, 'vit': 'vit_base_patch32_384', 'hidden_size': 768, 'num_heads': 12, 'num_layers': 12, 'mlp_ratio': 4, 'drop_rate': 0.1, 'optim_type': 'adamw', 'learning_rate': 0.0001, 'weight_decay': 0.01, 'decay_power': 1, 'max_epoch': 10, 'max_steps': 1, 'warmup_steps': 0.1, 'end_lr': 0, 'lr_mult': 1, 'get_recall_metric': False, 'resume_from': None, 'fast_dev_run': False, 'val_check_interval': 1.0, 'test_only': True, 'data_root': '/data-4/users/mileriso/datasets/OOD/arrows', 'log_dir': 'result', 'per_gpu_batchsize': 64, 'num_gpus': 1, 'num_nodes': 1, 'load_path': '/data-4/users/mileriso/models/vilt_nlvr2.ckpt', 'num_workers': 8, 'precision': 32}\n",
    "vilt_config_vqav2 = {'exp_name': 'test_ood_vqa', 'seed': 0, 'datasets': ['ood_vqa'], 'loss_names': {'itm': 0, 'mlm': 0, 'mpp': 0, 'vqa': 1, 'nlvr2': 0, 'irtr': 0}, 'batch_size': 256, 'accelerator': 'gpu', 'train_transform_keys': ['pixelbert_randaug'], 'val_transform_keys': ['pixelbert'], 'image_size': 384, 'max_image_len': -1, 'patch_size': 32, 'draw_false_image': 0, 'image_only': False, 'vqav2_label_size': 3129, 'max_text_len': 40, 'tokenizer': 'bert-base-uncased', 'vocab_size': 30522, 'whole_word_masking': False, 'mlm_prob': 0.15, 'draw_false_text': 0, 'vit': 'vit_base_patch32_384', 'hidden_size': 768, 'num_heads': 12, 'num_layers': 12, 'mlp_ratio': 4, 'drop_rate': 0.1, 'optim_type': 'adamw', 'learning_rate': 0.0001, 'weight_decay': 0.01, 'decay_power': 1, 'max_epoch': 10, 'max_steps': 1, 'warmup_steps': 0.1, 'end_lr': 0, 'lr_mult': 10, 'get_recall_metric': False, 'resume_from': None, 'fast_dev_run': False, 'val_check_interval': 0.1, 'test_only': True, 'data_root': '/data-4/users/mileriso/datasets/OOD/arrows', 'log_dir': 'result', 'per_gpu_batchsize': 64, 'num_gpus': 1, 'num_nodes': 1, 'load_path': '/data-4/users/mileriso/models/vilt_vqa.ckpt', 'num_workers': 8, 'precision': 32}\n",
    "\n",
    "meter_config_nlvr2 = {'exp_name': 'test_ood_nlvr2', 'seed': 0, 'datasets': ['ood_nlvr2'], 'loss_names': {'itm': 0, 'mlm': 0, 'mpp': 0, 'vqa': 0, 'vcr': 0, 'vcr_qar': 0, 'nlvr2': 1, 'irtr': 0, 'contras': 0, 'snli': 0}, 'batch_size': 256, 'accelerator': 'gpu', 'train_transform_keys': ['clip'], 'val_transform_keys': ['clip'], 'image_size': 288, 'patch_size': 16, 'draw_false_image': 0, 'image_only': False, 'resolution_before': 224, 'vqav2_label_size': 3129, 'max_text_len': 50, 'tokenizer': 'roberta-base', 'vocab_size': 50265, 'whole_word_masking': False, 'mlm_prob': 0.15, 'draw_false_text': 0, 'num_top_layer': 6, 'input_image_embed_size': 768, 'input_text_embed_size': 768, 'vit': 'ViT-B/16', 'hidden_size': 768, 'num_heads': 12, 'num_layers': 6, 'mlp_ratio': 4, 'drop_rate': 0.1, 'optim_type': 'adamw', 'learning_rate': 1e-05, 'weight_decay': 0.01, 'decay_power': 1, 'max_epoch': 10, 'max_steps': 1, 'warmup_steps': 0.1, 'end_lr': 0, 'lr_mult_head': 10, 'lr_mult_cross_modal': 5, 'get_recall_metric': False, 'resume_from': None, 'fast_dev_run': False, 'val_check_interval': 1.0, 'test_only': True, 'data_root': '/data-4/users/mileriso/datasets/OOD/arrows', 'log_dir': 'result', 'per_gpu_batchsize': 64, 'num_gpus': 1, 'num_nodes': 1, 'load_path': '/data-4/users/mileriso/models/meter_nlvr2.ckpt', 'num_workers': 8, 'precision': 32}\n",
    "meter_config_vqav2 = {'exp_name': 'test_ood_vqa', 'seed': 0, 'datasets': ['ood_vqa'], 'loss_names': {'itm': 0, 'mlm': 0, 'mpp': 0, 'vqa': 1, 'vcr': 0, 'vcr_qar': 0, 'nlvr2': 0, 'irtr': 0, 'contras': 0, 'snli': 0}, 'batch_size': 512, 'accelerator': 'gpu', 'train_transform_keys': ['clip'], 'val_transform_keys': ['clip'], 'image_size': 576, 'patch_size': 16, 'draw_false_image': 0, 'image_only': False, 'resolution_before': 224, 'vqav2_label_size': 3129, 'max_text_len': 50, 'tokenizer': 'roberta-base', 'vocab_size': 50265, 'whole_word_masking': False, 'mlm_prob': 0.15, 'draw_false_text': 0, 'num_top_layer': 6, 'input_image_embed_size': 768, 'input_text_embed_size': 768, 'vit': 'ViT-B/16', 'hidden_size': 768, 'num_heads': 12, 'num_layers': 6, 'mlp_ratio': 4, 'drop_rate': 0.1, 'optim_type': 'adamw', 'learning_rate': 5e-06, 'weight_decay': 0.01, 'decay_power': 1, 'max_epoch': 10, 'max_steps': 1, 'warmup_steps': 0.1, 'end_lr': 0, 'lr_mult_head': 50, 'lr_mult_cross_modal': 5, 'get_recall_metric': False, 'resume_from': None, 'fast_dev_run': False, 'val_check_interval': 0.1, 'test_only': True, 'data_root': '/data-4/users/mileriso/datasets/OOD/arrows', 'log_dir': 'result', 'per_gpu_batchsize': 4, 'num_gpus': 1, 'num_nodes': 1, 'load_path': '/data-4/users/mileriso/models/meter_vqa.ckpt', 'num_workers': 8, 'precision': 32}\n",
    "\n",
    "pl.seed_everything(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Datamodule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a child datamodule that constructs a smaller version of the full datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 13:26:40.207444: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733920000.224605  708907 cuda_dnn.cc:8498] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733920000.229933  708907 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "from vilt.datamodules.multitask_datamodule import MTDataModule as MTDataModuleVILT\n",
    "from meter.datamodules.multitask_datamodule import MTDataModule as MTDataModuleMeter\n",
    "\n",
    "class SmallMTDataModuleVILT(MTDataModuleVILT):\n",
    "    def __init__(self, _config, dist=False, num_samples=5, start_idx=100):\n",
    "        super().__init__(_config, dist)\n",
    "        self.num_samples = num_samples\n",
    "        self.start_idx = start_idx\n",
    "\n",
    "    def setup(self, stage):\n",
    "        super().setup(stage)\n",
    "        \n",
    "        # Limit the number of samples in the datasets\n",
    "        self.train_dataset = Subset(self.train_dataset, range(self.start_idx, self.start_idx+self.num_samples))\n",
    "        self.val_dataset = Subset(self.val_dataset, range(self.start_idx, self.start_idx+self.num_samples))\n",
    "        self.test_dataset = Subset(self.test_dataset, range(self.start_idx, self.start_idx+self.num_samples))\n",
    "\n",
    "class SmallMTDataModuleMETER(MTDataModuleMeter):\n",
    "    def __init__(self, _config, dist=False, num_samples=10, start_idx=100):\n",
    "        super().__init__(_config, dist)\n",
    "        self.num_samples = num_samples\n",
    "        self.start_idx = start_idx\n",
    "\n",
    "    def setup(self, stage):\n",
    "        super().setup(stage)\n",
    "        \n",
    "        # Limit the number of samples in the datasets\n",
    "        self.train_dataset = Subset(self.train_dataset, range(self.start_idx, self.start_idx+self.num_samples))\n",
    "        self.val_dataset = Subset(self.val_dataset, range(self.start_idx, self.start_idx+self.num_samples))\n",
    "        self.test_dataset = Subset(self.test_dataset, range(self.start_idx, self.start_idx+self.num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the configuration and initialize the test and full datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded names: ['vqa_vlue_test']\n",
      "Loaded names: ['vqa_vlue_test']\n",
      "Loaded names: ['vqa_vlue_test']\n"
     ]
    }
   ],
   "source": [
    "# Set the configuration\n",
    "_config = vilt_config_vqav2\n",
    "_config[\"model_\"] = \"vilt\"\n",
    "_config[\"batch_size\"] = 32\n",
    "\n",
    "# ==========================================\n",
    "# ========= Create full datamodule =========\n",
    "# ==========================================\n",
    "if \"meter\" in _config[\"model_\"]:\n",
    "    full_dm = MTDataModuleMeter(_config, dist=False)\n",
    "    \n",
    "    calibrarte_dm = SmallMTDataModuleMETER(_config, dist=False, num_samples=5, start_idx=100)\n",
    "    \n",
    "    infer_dm = SmallMTDataModuleMETER(_config, dist=False, num_samples=5, start_idx=0)\n",
    "    infer_dm.setup(\"test\")\n",
    "    infer_dataloader = infer_dm.test_dataloader()\n",
    "\n",
    "else:\n",
    "    full_dm = MTDataModuleVILT(_config, dist=False)\n",
    "\n",
    "    calibrarte_dm = SmallMTDataModuleVILT(_config, dist=False, num_samples=5, start_idx=100)\n",
    "    \n",
    "    infer_dm = SmallMTDataModuleVILT(_config, dist=False, num_samples=5, start_idx=0)\n",
    "    infer_dm.setup(\"test\")\n",
    "    infer_dataloader = infer_dm.test_dataloader()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized ViLT model\n"
     ]
    }
   ],
   "source": [
    "from vilt.modules import ViLTransformerSS\n",
    "from meter.modules import METERTransformerSS\n",
    "\n",
    "if _config[\"model_\"] == \"vilt\":\n",
    "    model = ViLTransformerSS(_config)\n",
    "    print(\"Initialized ViLT model\")\n",
    "\n",
    "elif _config[\"model_\"] == \"meter\":\n",
    "    model = METERTransformerSS(_config)\n",
    "    print(\"Initialized METER model\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Model not supported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize The Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded names: ['vqa_vlue_test']\n",
      "Loaded names: ['vqa_vlue_test']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded names: ['vqa_vlue_test']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add16df2acf94f4795c2286195c4da00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 5. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('vqa/val/loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('vqa/val/score', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('vqa/val/score_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('vqa/val/loss_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val/the_metric', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      val/the_metric       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       vqa/val/loss        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    3.7881855964660645     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    vqa/val/loss_epoch     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    3.7881853580474854     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       vqa/val/score       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    vqa/val/score_epoch    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     val/the_metric      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      vqa/val/loss       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   3.7881855964660645    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   vqa/val/loss_epoch    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   3.7881853580474854    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      vqa/val/score      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   vqa/val/score_epoch   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'vqa/val/loss': 3.7881855964660645,\n",
       "  'vqa/val/score': 0.0,\n",
       "  'vqa/val/score_epoch': 0.0,\n",
       "  'vqa/val/loss_epoch': 3.7881853580474854,\n",
       "  'val/the_metric': 0.0}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========== Initialize the trainer for full precision ==========\n",
    "exp_name = f'{_config[\"exp_name\"]}'\n",
    "\n",
    "os.makedirs(_config[\"log_dir\"], exist_ok=True)\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val/the_metric\",\n",
    "    mode=\"max\",\n",
    "    save_last=True,\n",
    ")\n",
    "logger = pl.loggers.TensorBoardLogger(\n",
    "    _config[\"log_dir\"],\n",
    "    name=f'{exp_name}_seed{_config[\"seed\"]}_from_{_config[\"load_path\"].split(\"/\")[-1][:-5]}',\n",
    ")\n",
    "\n",
    "lr_callback = pl.callbacks.LearningRateMonitor(logging_interval=\"step\")\n",
    "callbacks = [checkpoint_callback, lr_callback]\n",
    "\n",
    "num_gpus = (\n",
    "    _config[\"num_gpus\"]\n",
    "    if isinstance(_config[\"num_gpus\"], int)\n",
    "    else len(_config[\"num_gpus\"])\n",
    ")\n",
    "\n",
    "grad_steps = _config[\"batch_size\"] // (\n",
    "    _config[\"per_gpu_batchsize\"] * num_gpus * _config[\"num_nodes\"]\n",
    ")\n",
    "\n",
    "max_steps = _config[\"max_steps\"] if _config[\"max_steps\"] is not None else None\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "        accelerator=\"cpu\",\n",
    "        devices=1,\n",
    "        num_nodes=_config[\"num_nodes\"],\n",
    "        precision=_config[\"precision\"],\n",
    "        # strategy=\"ddp\",\n",
    "        benchmark=True,\n",
    "        deterministic=False,\n",
    "        max_epochs=_config[\"max_epoch\"] if max_steps is None else 1000,\n",
    "        max_steps=max_steps,\n",
    "        callbacks=callbacks,\n",
    "        logger=logger,\n",
    "        # accumulate_grad_batches=grad_steps,\n",
    "        log_every_n_steps=10,\n",
    "        fast_dev_run=_config[\"fast_dev_run\"],\n",
    "        val_check_interval=_config[\"val_check_interval\"],\n",
    "    )\n",
    "\n",
    "trainer.test(model, datamodule=calibrarte_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization | PTQ to 8-bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_dynamic = copy.deepcopy(model)\n",
    "\n",
    "# torch.quantization.quantize_dynamic(\n",
    "#         default_dynamic, {torch.nn.Embedding}, dtype=torch.quint8, inplace=True\n",
    "#     )\n",
    "\n",
    "# torch.quantization.quantize_dynamic(\n",
    "#         default_dynamic, {torch.nn.Linear, torch.nn.LayerNorm}, dtype=torch.qint8, inplace=True\n",
    "#     )\n",
    "\n",
    "# print(\"Size after quantization:\")\n",
    "# print_size_of_model(default_dynamic)\n",
    "# # print(model_dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Quantizing the model DYNAMIC =========\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Embedding quantization is only supported with float_qparams_weight_only_qconfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdynamic_quantization\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdq\u001b[39;00m\n\u001b[1;32m      3\u001b[0m custom_8bit \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(model)\n\u001b[0;32m----> 4\u001b[0m custom_8bit \u001b[38;5;241m=\u001b[39m \u001b[43mdq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize_model_dynamic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcustom_8bit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize after quantization:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m print_size_of_model(custom_8bit)\n",
      "File \u001b[0;32m~/thesis/dynamic_quantization.py:70\u001b[0m, in \u001b[0;36mquantize_model_dynamic\u001b[0;34m(model, precision)\u001b[0m\n\u001b[1;32m     66\u001b[0m _model \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(model)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m======== Quantizing the model DYNAMIC =========\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 70\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize_dynamic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_qconfig\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquint8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     72\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m torch\u001b[38;5;241m.\u001b[39mquantization\u001b[38;5;241m.\u001b[39mquantize_dynamic(\n\u001b[1;32m     75\u001b[0m     _model, {torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear: quantization_config, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLayerNorm: quantization_config}, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mqint8, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     76\u001b[0m )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _model\n",
      "File \u001b[0;32m/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/torch/ao/quantization/quantize.py:564\u001b[0m, in \u001b[0;36mquantize_dynamic\u001b[0;34m(model, qconfig_spec, dtype, mapping, inplace)\u001b[0m\n\u001b[1;32m    562\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    563\u001b[0m propagate_qconfig_(model, qconfig_spec)\n\u001b[0;32m--> 564\u001b[0m \u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/torch/ao/quantization/quantize.py:661\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[0m\n\u001b[1;32m    658\u001b[0m     module \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(module)\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# print(f\"Converting module: {module}\")\u001b[39;00m\n\u001b[0;32m--> 661\u001b[0m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_qconfig:\n\u001b[1;32m    670\u001b[0m     _remove_qconfig(module)\n",
      "File \u001b[0;32m/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/torch/ao/quantization/quantize.py:718\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    715\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule)\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping\n\u001b[1;32m    717\u001b[0m     ):\n\u001b[0;32m--> 718\u001b[0m         \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# inplace\u001b[39;49;00m\n\u001b[1;32m    722\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m swap_module(\n\u001b[1;32m    727\u001b[0m         mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant\n\u001b[1;32m    728\u001b[0m     )\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/torch/ao/quantization/quantize.py:726\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    715\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule)\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping\n\u001b[1;32m    717\u001b[0m     ):\n\u001b[1;32m    718\u001b[0m         _convert(\n\u001b[1;32m    719\u001b[0m             mod,\n\u001b[1;32m    720\u001b[0m             mapping,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    724\u001b[0m             use_precomputed_fake_quant\u001b[38;5;241m=\u001b[39muse_precomputed_fake_quant,\n\u001b[1;32m    725\u001b[0m         )\n\u001b[0;32m--> 726\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m \u001b[43mswap_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_module_class_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    731\u001b[0m     module\u001b[38;5;241m.\u001b[39m_modules[key] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/torch/ao/quantization/quantize.py:768\u001b[0m, in \u001b[0;36mswap_module\u001b[0;34m(mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant)\u001b[0m\n\u001b[1;32m    766\u001b[0m sig \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(qmod\u001b[38;5;241m.\u001b[39mfrom_float)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_precomputed_fake_quant\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sig\u001b[38;5;241m.\u001b[39mparameters:\n\u001b[0;32m--> 768\u001b[0m     new_mod \u001b[38;5;241m=\u001b[39m \u001b[43mqmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_float\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_precomputed_fake_quant\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     new_mod \u001b[38;5;241m=\u001b[39m qmod\u001b[38;5;241m.\u001b[39mfrom_float(mod)\n",
      "File \u001b[0;32m/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/torch/ao/nn/quantized/modules/embedding_ops.py:229\u001b[0m, in \u001b[0;36mEmbedding.from_float\u001b[0;34m(cls, mod, use_precomputed_fake_quant)\u001b[0m\n\u001b[1;32m    225\u001b[0m dtype \u001b[38;5;241m=\u001b[39m weight_observer\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    226\u001b[0m is_float_qparams_qconfig \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    227\u001b[0m     weight_observer\u001b[38;5;241m.\u001b[39mqscheme \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mper_channel_affine_float_qparams\n\u001b[1;32m    228\u001b[0m )\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    230\u001b[0m     is_float_qparams_qconfig\n\u001b[1;32m    231\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding quantization is only supported with float_qparams_weight_only_qconfig.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    234\u001b[0m     dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mquint8 \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mquint4x2\n\u001b[1;32m    235\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe only supported dtype for nnq.Embedding is torch.quint8 and torch.quint4x2, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Run the observer to calculate qparams.\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Embedding quantization is only supported with float_qparams_weight_only_qconfig."
     ]
    }
   ],
   "source": [
    "import dynamic_quantization as dq\n",
    "\n",
    "custom_8bit = copy.deepcopy(model)\n",
    "custom_8bit = dq.quantize_model_dynamic(custom_8bit, 8)\n",
    "\n",
    "print(\"Size after quantization:\")\n",
    "print_size_of_model(custom_8bit)\n",
    "# print(model_dynamic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c8c94691e84c9eb66dfe89e26e46bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      val/the_metric       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       vqa/val/loss        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     3.790861129760742     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    vqa/val/loss_epoch     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     3.790861129760742     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       vqa/val/score       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    vqa/val/score_epoch    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     val/the_metric      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      vqa/val/loss       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    3.790861129760742    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   vqa/val/loss_epoch    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    3.790861129760742    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      vqa/val/score      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   vqa/val/score_epoch   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after quantization:\n",
      "Size (MB): 120.119778\n"
     ]
    }
   ],
   "source": [
    "# model_static = copy.deepcopy(model)\n",
    "\n",
    "# # General quantization configuration\n",
    "# quantization_config = torch.quantization.get_default_qconfig('x86')\n",
    "\n",
    "# # Configuration for nn.Embedding layers\n",
    "# embedding_qconfig = torch.quantization.QConfig(\n",
    "#     activation=torch.quantization.HistogramObserver,\n",
    "#     weight=torch.quantization.default_float_qparams_observer.with_args(dtype=torch.quint8)\n",
    "# )\n",
    "\n",
    "# if _config[\"model_\"] == \"vilt\":\n",
    "#     # Assign the quantization configurations to the model\n",
    "#     model_static.qconfig = quantization_config\n",
    "#     model_static.token_type_embeddings.qconfig = embedding_qconfig\n",
    "#     model_static.text_embeddings.word_embeddings.qconfig = embedding_qconfig\n",
    "#     model_static.text_embeddings.position_embeddings.qconfig = embedding_qconfig\n",
    "#     model_static.text_embeddings.token_type_embeddings.qconfig = embedding_qconfig\n",
    "\n",
    "# elif _config[\"model_\"] == \"meter\":\n",
    "#     # Assign the quantization configurations to the model\n",
    "#     model_static.qconfig = quantization_config\n",
    "#     model_static.token_type_embeddings.qconfig = embedding_qconfig\n",
    "#     model_static.text_transformer.embeddings.word_embeddings.qconfig = embedding_qconfig\n",
    "#     model_static.text_transformer.embeddings.position_embeddings.qconfig = embedding_qconfig\n",
    "#     model_static.text_transformer.embeddings.token_type_embeddings.qconfig = embedding_qconfig\n",
    "\n",
    "# # Perform static quantization\n",
    "# torch.quantization.prepare(model_static, inplace=True)\n",
    "# trainer.test(model_static, datamodule=calibrarte_dm)\n",
    "# torch.quantization.convert(model_static, inplace=True)\n",
    "\n",
    "# print(\"Size after quantization:\")\n",
    "# print_size_of_model(model_static)\n",
    "# # print(model_static)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numeric Suite Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization._numeric_suite as ns\n",
    "\n",
    "def compute_error(x, y):\n",
    "    \"\"\"\n",
    "    Signal to Noise Ratio (SNR)    \n",
    "    \"\"\"\n",
    "    Ps = torch.norm(x)\n",
    "    Pn = torch.norm(x-y)\n",
    "    return 20*torch.log10(Ps/Pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_batch = next(iter(infer_dataloader))\n",
    "calibration_batch = next(iter(calibrarte_dm.val_dataloader()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Model - Infer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys of wt_compare_dict:\n",
      "dict_keys(['text_embeddings.LayerNorm.weight', 'transformer.patch_embed.proj.weight', 'transformer.blocks.0.norm1.weight', 'transformer.blocks.0.attn.qkv._packed_params._packed_params', 'transformer.blocks.0.attn.proj._packed_params._packed_params', 'transformer.blocks.0.norm2.weight', 'transformer.blocks.0.mlp.fc1._packed_params._packed_params', 'transformer.blocks.0.mlp.fc2._packed_params._packed_params', 'transformer.blocks.1.norm1.weight', 'transformer.blocks.1.attn.qkv._packed_params._packed_params', 'transformer.blocks.1.attn.proj._packed_params._packed_params', 'transformer.blocks.1.norm2.weight', 'transformer.blocks.1.mlp.fc1._packed_params._packed_params', 'transformer.blocks.1.mlp.fc2._packed_params._packed_params', 'transformer.blocks.2.norm1.weight', 'transformer.blocks.2.attn.qkv._packed_params._packed_params', 'transformer.blocks.2.attn.proj._packed_params._packed_params', 'transformer.blocks.2.norm2.weight', 'transformer.blocks.2.mlp.fc1._packed_params._packed_params', 'transformer.blocks.2.mlp.fc2._packed_params._packed_params', 'transformer.blocks.3.norm1.weight', 'transformer.blocks.3.attn.qkv._packed_params._packed_params', 'transformer.blocks.3.attn.proj._packed_params._packed_params', 'transformer.blocks.3.norm2.weight', 'transformer.blocks.3.mlp.fc1._packed_params._packed_params', 'transformer.blocks.3.mlp.fc2._packed_params._packed_params', 'transformer.blocks.4.norm1.weight', 'transformer.blocks.4.attn.qkv._packed_params._packed_params', 'transformer.blocks.4.attn.proj._packed_params._packed_params', 'transformer.blocks.4.norm2.weight', 'transformer.blocks.4.mlp.fc1._packed_params._packed_params', 'transformer.blocks.4.mlp.fc2._packed_params._packed_params', 'transformer.blocks.5.norm1.weight', 'transformer.blocks.5.attn.qkv._packed_params._packed_params', 'transformer.blocks.5.attn.proj._packed_params._packed_params', 'transformer.blocks.5.norm2.weight', 'transformer.blocks.5.mlp.fc1._packed_params._packed_params', 'transformer.blocks.5.mlp.fc2._packed_params._packed_params', 'transformer.blocks.6.norm1.weight', 'transformer.blocks.6.attn.qkv._packed_params._packed_params', 'transformer.blocks.6.attn.proj._packed_params._packed_params', 'transformer.blocks.6.norm2.weight', 'transformer.blocks.6.mlp.fc1._packed_params._packed_params', 'transformer.blocks.6.mlp.fc2._packed_params._packed_params', 'transformer.blocks.7.norm1.weight', 'transformer.blocks.7.attn.qkv._packed_params._packed_params', 'transformer.blocks.7.attn.proj._packed_params._packed_params', 'transformer.blocks.7.norm2.weight', 'transformer.blocks.7.mlp.fc1._packed_params._packed_params', 'transformer.blocks.7.mlp.fc2._packed_params._packed_params', 'transformer.blocks.8.norm1.weight', 'transformer.blocks.8.attn.qkv._packed_params._packed_params', 'transformer.blocks.8.attn.proj._packed_params._packed_params', 'transformer.blocks.8.norm2.weight', 'transformer.blocks.8.mlp.fc1._packed_params._packed_params', 'transformer.blocks.8.mlp.fc2._packed_params._packed_params', 'transformer.blocks.9.norm1.weight', 'transformer.blocks.9.attn.qkv._packed_params._packed_params', 'transformer.blocks.9.attn.proj._packed_params._packed_params', 'transformer.blocks.9.norm2.weight', 'transformer.blocks.9.mlp.fc1._packed_params._packed_params', 'transformer.blocks.9.mlp.fc2._packed_params._packed_params', 'transformer.blocks.10.norm1.weight', 'transformer.blocks.10.attn.qkv._packed_params._packed_params', 'transformer.blocks.10.attn.proj._packed_params._packed_params', 'transformer.blocks.10.norm2.weight', 'transformer.blocks.10.mlp.fc1._packed_params._packed_params', 'transformer.blocks.10.mlp.fc2._packed_params._packed_params', 'transformer.blocks.11.norm1.weight', 'transformer.blocks.11.attn.qkv._packed_params._packed_params', 'transformer.blocks.11.attn.proj._packed_params._packed_params', 'transformer.blocks.11.norm2.weight', 'transformer.blocks.11.mlp.fc1._packed_params._packed_params', 'transformer.blocks.11.mlp.fc2._packed_params._packed_params', 'transformer.norm.weight', 'pooler.dense._packed_params._packed_params', 'vqa_classifier.0._packed_params._packed_params', 'vqa_classifier.1.weight', 'vqa_classifier.3._packed_params._packed_params'])\n",
      "text_embeddings.LayerNorm.weight tensor(inf)\n",
      "transformer.patch_embed.proj.weight tensor(39.6005)\n",
      "transformer.blocks.0.norm1.weight tensor(inf)\n",
      "transformer.blocks.0.attn.qkv._packed_params._packed_params tensor(39.7125)\n",
      "transformer.blocks.0.attn.proj._packed_params._packed_params tensor(39.1381)\n",
      "transformer.blocks.0.norm2.weight tensor(inf)\n",
      "transformer.blocks.0.mlp.fc1._packed_params._packed_params tensor(39.3022)\n",
      "transformer.blocks.0.mlp.fc2._packed_params._packed_params tensor(36.9442)\n",
      "transformer.blocks.1.norm1.weight tensor(inf)\n",
      "transformer.blocks.1.attn.qkv._packed_params._packed_params tensor(40.3811)\n",
      "transformer.blocks.1.attn.proj._packed_params._packed_params tensor(40.6048)\n",
      "transformer.blocks.1.norm2.weight tensor(inf)\n",
      "transformer.blocks.1.mlp.fc1._packed_params._packed_params tensor(39.7662)\n",
      "transformer.blocks.1.mlp.fc2._packed_params._packed_params tensor(37.0412)\n",
      "transformer.blocks.2.norm1.weight tensor(inf)\n",
      "transformer.blocks.2.attn.qkv._packed_params._packed_params tensor(41.0412)\n",
      "transformer.blocks.2.attn.proj._packed_params._packed_params tensor(41.7073)\n",
      "transformer.blocks.2.norm2.weight tensor(inf)\n",
      "transformer.blocks.2.mlp.fc1._packed_params._packed_params tensor(41.1843)\n",
      "transformer.blocks.2.mlp.fc2._packed_params._packed_params tensor(37.5923)\n",
      "transformer.blocks.3.norm1.weight tensor(inf)\n",
      "transformer.blocks.3.attn.qkv._packed_params._packed_params tensor(41.3750)\n",
      "transformer.blocks.3.attn.proj._packed_params._packed_params tensor(42.0514)\n",
      "transformer.blocks.3.norm2.weight tensor(inf)\n",
      "transformer.blocks.3.mlp.fc1._packed_params._packed_params tensor(41.6692)\n",
      "transformer.blocks.3.mlp.fc2._packed_params._packed_params tensor(39.7913)\n",
      "transformer.blocks.4.norm1.weight tensor(inf)\n",
      "transformer.blocks.4.attn.qkv._packed_params._packed_params tensor(41.5467)\n",
      "transformer.blocks.4.attn.proj._packed_params._packed_params tensor(42.1508)\n",
      "transformer.blocks.4.norm2.weight tensor(inf)\n",
      "transformer.blocks.4.mlp.fc1._packed_params._packed_params tensor(41.9596)\n",
      "transformer.blocks.4.mlp.fc2._packed_params._packed_params tensor(39.5251)\n",
      "transformer.blocks.5.norm1.weight tensor(inf)\n",
      "transformer.blocks.5.attn.qkv._packed_params._packed_params tensor(41.7084)\n",
      "transformer.blocks.5.attn.proj._packed_params._packed_params tensor(42.1632)\n",
      "transformer.blocks.5.norm2.weight tensor(inf)\n",
      "transformer.blocks.5.mlp.fc1._packed_params._packed_params tensor(42.0419)\n",
      "transformer.blocks.5.mlp.fc2._packed_params._packed_params tensor(39.2079)\n",
      "transformer.blocks.6.norm1.weight tensor(inf)\n",
      "transformer.blocks.6.attn.qkv._packed_params._packed_params tensor(41.9280)\n",
      "transformer.blocks.6.attn.proj._packed_params._packed_params tensor(42.1906)\n",
      "transformer.blocks.6.norm2.weight tensor(inf)\n",
      "transformer.blocks.6.mlp.fc1._packed_params._packed_params tensor(42.1140)\n",
      "transformer.blocks.6.mlp.fc2._packed_params._packed_params tensor(37.7056)\n",
      "transformer.blocks.7.norm1.weight tensor(inf)\n",
      "transformer.blocks.7.attn.qkv._packed_params._packed_params tensor(41.9029)\n",
      "transformer.blocks.7.attn.proj._packed_params._packed_params tensor(42.2536)\n",
      "transformer.blocks.7.norm2.weight tensor(inf)\n",
      "transformer.blocks.7.mlp.fc1._packed_params._packed_params tensor(42.0312)\n",
      "transformer.blocks.7.mlp.fc2._packed_params._packed_params tensor(37.1339)\n",
      "transformer.blocks.8.norm1.weight tensor(inf)\n",
      "transformer.blocks.8.attn.qkv._packed_params._packed_params tensor(42.0158)\n",
      "transformer.blocks.8.attn.proj._packed_params._packed_params tensor(42.3573)\n",
      "transformer.blocks.8.norm2.weight tensor(inf)\n",
      "transformer.blocks.8.mlp.fc1._packed_params._packed_params tensor(41.6929)\n",
      "transformer.blocks.8.mlp.fc2._packed_params._packed_params tensor(38.2527)\n",
      "transformer.blocks.9.norm1.weight tensor(inf)\n",
      "transformer.blocks.9.attn.qkv._packed_params._packed_params tensor(41.8669)\n",
      "transformer.blocks.9.attn.proj._packed_params._packed_params tensor(42.2721)\n",
      "transformer.blocks.9.norm2.weight tensor(inf)\n",
      "transformer.blocks.9.mlp.fc1._packed_params._packed_params tensor(42.0395)\n",
      "transformer.blocks.9.mlp.fc2._packed_params._packed_params tensor(38.6189)\n",
      "transformer.blocks.10.norm1.weight tensor(inf)\n",
      "transformer.blocks.10.attn.qkv._packed_params._packed_params tensor(41.4728)\n",
      "transformer.blocks.10.attn.proj._packed_params._packed_params tensor(42.2866)\n",
      "transformer.blocks.10.norm2.weight tensor(inf)\n",
      "transformer.blocks.10.mlp.fc1._packed_params._packed_params tensor(42.0577)\n",
      "transformer.blocks.10.mlp.fc2._packed_params._packed_params tensor(39.8160)\n",
      "transformer.blocks.11.norm1.weight tensor(inf)\n",
      "transformer.blocks.11.attn.qkv._packed_params._packed_params tensor(41.7699)\n",
      "transformer.blocks.11.attn.proj._packed_params._packed_params tensor(41.9149)\n",
      "transformer.blocks.11.norm2.weight tensor(inf)\n",
      "transformer.blocks.11.mlp.fc1._packed_params._packed_params tensor(42.0330)\n",
      "transformer.blocks.11.mlp.fc2._packed_params._packed_params tensor(38.6082)\n",
      "transformer.norm.weight tensor(inf)\n",
      "pooler.dense._packed_params._packed_params tensor(42.2156)\n",
      "vqa_classifier.0._packed_params._packed_params tensor(42.0574)\n",
      "vqa_classifier.1.weight tensor(inf)\n",
      "vqa_classifier.3._packed_params._packed_params tensor(32.1249)\n"
     ]
    }
   ],
   "source": [
    "# # ======== Static quantization comparison ========\n",
    "# wt_compare_dict_static = ns.compare_weights(model.state_dict(), model_static.state_dict())\n",
    "\n",
    "# # print('keys of wt_compare_dict:')\n",
    "# # print(wt_compare_dict_static.keys())\n",
    "\n",
    "# # key = 'text_embeddings.LayerNorm.weight'\n",
    "\n",
    "# # print(f\"\\nkeys of wt_compare_dict entry for {key} weight:\")\n",
    "# # print(wt_compare_dict_static[key].keys())\n",
    "# # print(wt_compare_dict_static[key]['float'].shape)\n",
    "# # print(wt_compare_dict_static[key]['quantized'].shape)\n",
    "\n",
    "# for key in wt_compare_dict_static:\n",
    "#     print(key, compute_error(wt_compare_dict_static[key]['float'], wt_compare_dict_static[key]['quantized'].dequantize()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/pytorch_lightning/core/module.py:445: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'quantized::conv2d.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d.new' is only available for these backends: [XLA, Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastXPU, AutocastXLA, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nXLA: registered at torch_xla/csrc/aten_fallback.cpp:460 [backend fallback]\nMeta: registered at ../aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]\nQuantizedCPU: registered at ../aten/src/ATen/native/quantized/cpu/qconv.cpp:1972 [kernel]\nQuantizedCUDA: registered at ../aten/src/ATen/native/quantized/cudnn/Conv.cpp:391 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:349 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:96 [backend fallback]\nAutogradOther: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradCPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradCUDA: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:75 [backend fallback]\nAutogradXLA: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:79 [backend fallback]\nAutogradMPS: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:87 [backend fallback]\nAutogradXPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:71 [backend fallback]\nAutogradHPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:100 [backend fallback]\nAutogradLazy: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:83 [backend fallback]\nAutogradMeta: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:91 [backend fallback]\nTracer: registered at ../torch/csrc/autograd/TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:321 [backend fallback]\nAutocastXPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:463 [backend fallback]\nAutocastXLA: fallthrough registered at torch_xla/csrc/autocast_mode.cpp:25 [backend fallback]\nAutocastMPS: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Take in floating point and quantized model as well as input data, and returns a dict, with keys\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# corresponding to the quantized module names and each entry being a dictionary with two keys 'float' and\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 'quantized', containing the activations of floating point and quantized model at matching locations.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# act_compare_dict_static = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(model_static), full_batch)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m act_compare_dict_static \u001b[38;5;241m=\u001b[39m \u001b[43mns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompare_model_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_static\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeys of act_compare_dict:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(act_compare_dict_static\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/torch/ao/ns/_numeric_suite.py:561\u001b[0m, in \u001b[0;36mcompare_model_outputs\u001b[0;34m(float_model, q_model, logger_cls, allow_list, *data)\u001b[0m\n\u001b[1;32m    559\u001b[0m prepare_model_outputs(float_model, q_model, logger_cls, allow_list)\n\u001b[1;32m    560\u001b[0m float_model(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m--> 561\u001b[0m \u001b[43mq_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m act_compare_dict \u001b[38;5;241m=\u001b[39m get_matching_activations(float_model, q_model)\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m act_compare_dict\n",
      "File \u001b[0;32m/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/thesis/vilt/modules/vilt_module.py:208\u001b[0m, in \u001b[0;36mViLTransformerSS.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# Visual Question Answering\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvqa\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_tasks:\n\u001b[0;32m--> 208\u001b[0m     ret\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mobjectives\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_vqa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# Natural Language for Visual Reasoning 2\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlvr2\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_tasks:\n",
      "File \u001b[0;32m~/thesis/vilt/modules/objectives.py:303\u001b[0m, in \u001b[0;36mcompute_vqa\u001b[0;34m(pl_module, batch)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_vqa\u001b[39m(pl_module, batch):\n\u001b[0;32m--> 303\u001b[0m     infer \u001b[38;5;241m=\u001b[39m \u001b[43mpl_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m     vqa_logits \u001b[38;5;241m=\u001b[39m pl_module\u001b[38;5;241m.\u001b[39mvqa_classifier(infer[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls_feats\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    305\u001b[0m     vqa_targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;28mlen\u001b[39m(vqa_logits), pl_module\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvqav2_label_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    307\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(pl_module\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/thesis/vilt/modules/vilt_module.py:139\u001b[0m, in \u001b[0;36mViLTransformerSS.infer\u001b[0;34m(self, batch, mask_text, mask_image, image_token_type_idx, image_embeds, image_masks)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m image_masks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     img \u001b[38;5;241m=\u001b[39m batch[imgkey][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    134\u001b[0m     (\n\u001b[1;32m    135\u001b[0m         image_embeds,\n\u001b[1;32m    136\u001b[0m         image_masks,\n\u001b[1;32m    137\u001b[0m         patch_index,\n\u001b[1;32m    138\u001b[0m         image_labels,\n\u001b[0;32m--> 139\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual_embed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_image_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_image_len\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_it\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     patch_index, image_labels \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    148\u001b[0m     )\n",
      "File \u001b[0;32m~/thesis/vilt/modules/vision_transformer.py:560\u001b[0m, in \u001b[0;36mVisionTransformer.visual_embed\u001b[0;34m(self, _x, max_image_len, mask_it)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisual_embed\u001b[39m(\u001b[38;5;28mself\u001b[39m, _x, max_image_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, mask_it\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;66;03m# _, _, ph, pw = self.patch_embed.proj.weight.shape\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m     x_mask \u001b[38;5;241m=\u001b[39m (_x\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()[:, \u001b[38;5;28;01mNone\u001b[39;00m, :, :]\n\u001b[1;32m    562\u001b[0m     x_mask \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(x_mask, size\u001b[38;5;241m=\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]))\u001b[38;5;241m.\u001b[39mlong()\n",
      "File \u001b[0;32m/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/thesis/vilt/modules/vision_transformer.py:407\u001b[0m, in \u001b[0;36mPatchEmbed.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    405\u001b[0m B, C, H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    406\u001b[0m \u001b[38;5;66;03m# FIXME look at relaxing size constraints\u001b[39;00m\n\u001b[0;32m--> 407\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1790\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1787\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1788\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1790\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1792\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1793\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1794\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1795\u001b[0m     ):\n\u001b[1;32m   1796\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/torch/ao/nn/quantized/modules/conv.py:595\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    591\u001b[0m     _reversed_padding_repeated_twice \u001b[38;5;241m=\u001b[39m _reverse_repeat_padding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding)\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    593\u001b[0m         \u001b[38;5;28minput\u001b[39m, _reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m    594\u001b[0m     )\n\u001b[0;32m--> 595\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_packed_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_point\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/torch/_ops.py:1116\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[0;32m-> 1116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'quantized::conv2d.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d.new' is only available for these backends: [XLA, Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastXPU, AutocastXLA, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nXLA: registered at torch_xla/csrc/aten_fallback.cpp:460 [backend fallback]\nMeta: registered at ../aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]\nQuantizedCPU: registered at ../aten/src/ATen/native/quantized/cpu/qconv.cpp:1972 [kernel]\nQuantizedCUDA: registered at ../aten/src/ATen/native/quantized/cudnn/Conv.cpp:391 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:349 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:96 [backend fallback]\nAutogradOther: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradCPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradCUDA: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:75 [backend fallback]\nAutogradXLA: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:79 [backend fallback]\nAutogradMPS: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:87 [backend fallback]\nAutogradXPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:71 [backend fallback]\nAutogradHPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:100 [backend fallback]\nAutogradLazy: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:83 [backend fallback]\nAutogradMeta: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:91 [backend fallback]\nTracer: registered at ../torch/csrc/autograd/TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:321 [backend fallback]\nAutocastXPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:463 [backend fallback]\nAutocastXLA: fallthrough registered at torch_xla/csrc/autocast_mode.cpp:25 [backend fallback]\nAutocastMPS: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "# # Take in floating point and quantized model as well as input data, and returns a dict, with keys\n",
    "# # corresponding to the quantized module names and each entry being a dictionary with two keys 'float' and\n",
    "# # 'quantized', containing the activations of floating point and quantized model at matching locations.\n",
    "\n",
    "\n",
    "# # act_compare_dict_static = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(model_static), full_batch)\n",
    "# act_compare_dict_static = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(model_static), infer_batch)\n",
    "\n",
    "# print('keys of act_compare_dict:')\n",
    "# print(act_compare_dict_static.keys())\n",
    "\n",
    "# key_act = \"transformer.blocks.0.attn.qkv.stats\"\n",
    "\n",
    "# print(f\"\\nkeys of act_compare_dict entry for {key} output:\")\n",
    "# print(act_compare_dict_static[key_act].keys())\n",
    "# print(act_compare_dict_static[key_act]['float'][0].shape)\n",
    "# print(act_compare_dict_static[key_act]['quantized'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - text_embeddings.LayerNorm.stats\n",
      "0 - 17.82866859436035\n",
      "1 - text_embeddings.quant.stats\n",
      "1 - 18.955486297607422\n",
      "2 - transformer.patch_embed.proj.stats\n",
      "2 - 26.32302474975586\n",
      "3 - transformer.patch_embed.quant.stats\n",
      "3 - 44.364158630371094\n",
      "4 - transformer.blocks.0.norm1.stats\n",
      "4 - 0.4864632785320282\n",
      "5 - transformer.blocks.0.attn.qkv.stats\n",
      "5 - 5.93009090423584\n",
      "6 - transformer.blocks.0.attn.proj.stats\n",
      "6 - 0.6434845328330994\n",
      "7 - transformer.blocks.0.attn.quant.stats\n",
      "7 - 0.4597845673561096\n",
      "8 - transformer.blocks.0.norm2.stats\n",
      "8 - 0.7093193531036377\n",
      "9 - transformer.blocks.0.mlp.fc1.stats\n",
      "9 - 6.044439315795898\n",
      "10 - transformer.blocks.0.mlp.fc2.stats\n",
      "10 - -1.2007348537445068\n",
      "11 - transformer.blocks.0.quant.stats\n",
      "11 - -0.07495744526386261\n",
      "12 - transformer.blocks.1.norm1.stats\n",
      "12 - 0.08032231032848358\n",
      "13 - transformer.blocks.1.attn.qkv.stats\n",
      "13 - 5.3930511474609375\n",
      "14 - transformer.blocks.1.attn.proj.stats\n",
      "14 - 0.07086136937141418\n",
      "15 - transformer.blocks.1.attn.quant.stats\n",
      "15 - 0.02302350290119648\n",
      "16 - transformer.blocks.1.norm2.stats\n",
      "16 - 0.9033985137939453\n",
      "17 - transformer.blocks.1.mlp.fc1.stats\n",
      "17 - 5.248117446899414\n",
      "18 - transformer.blocks.1.mlp.fc2.stats\n",
      "18 - -1.2541682720184326\n",
      "19 - transformer.blocks.1.quant.stats\n",
      "19 - 0.24195103347301483\n",
      "20 - transformer.blocks.2.norm1.stats\n",
      "20 - -0.05466576665639877\n",
      "21 - transformer.blocks.2.attn.qkv.stats\n",
      "21 - 2.0831074714660645\n",
      "22 - transformer.blocks.2.attn.proj.stats\n",
      "22 - -0.3821668326854706\n",
      "23 - transformer.blocks.2.attn.quant.stats\n",
      "23 - -0.08510462194681168\n",
      "24 - transformer.blocks.2.norm2.stats\n",
      "24 - 0.8347734808921814\n",
      "25 - transformer.blocks.2.mlp.fc1.stats\n",
      "25 - 5.62865686416626\n",
      "26 - transformer.blocks.2.mlp.fc2.stats\n",
      "26 - -1.147688388824463\n",
      "27 - transformer.blocks.2.quant.stats\n",
      "27 - 0.3492593467235565\n",
      "28 - transformer.blocks.3.norm1.stats\n",
      "28 - -0.1902659833431244\n",
      "29 - transformer.blocks.3.attn.qkv.stats\n",
      "29 - 2.30998158454895\n",
      "30 - transformer.blocks.3.attn.proj.stats\n",
      "30 - -0.23577958345413208\n",
      "31 - transformer.blocks.3.attn.quant.stats\n",
      "31 - -0.207035630941391\n",
      "32 - transformer.blocks.3.norm2.stats\n",
      "32 - 0.7336603999137878\n",
      "33 - transformer.blocks.3.mlp.fc1.stats\n",
      "33 - 5.759654998779297\n",
      "34 - transformer.blocks.3.mlp.fc2.stats\n",
      "34 - -1.1039509773254395\n",
      "35 - transformer.blocks.3.quant.stats\n",
      "35 - 0.32300424575805664\n",
      "36 - transformer.blocks.4.norm1.stats\n",
      "36 - -0.2526395618915558\n",
      "37 - transformer.blocks.4.attn.qkv.stats\n",
      "37 - 2.7585434913635254\n",
      "38 - transformer.blocks.4.attn.proj.stats\n",
      "38 - 0.08012533187866211\n",
      "39 - transformer.blocks.4.attn.quant.stats\n",
      "39 - -0.27051714062690735\n",
      "40 - transformer.blocks.4.norm2.stats\n",
      "40 - 0.6016798615455627\n",
      "41 - transformer.blocks.4.mlp.fc1.stats\n",
      "41 - 5.95241641998291\n",
      "42 - transformer.blocks.4.mlp.fc2.stats\n",
      "42 - -0.7656479477882385\n",
      "43 - transformer.blocks.4.quant.stats\n",
      "43 - 0.2588319778442383\n",
      "44 - transformer.blocks.5.norm1.stats\n",
      "44 - -0.2931104898452759\n",
      "45 - transformer.blocks.5.attn.qkv.stats\n",
      "45 - 3.2176218032836914\n",
      "46 - transformer.blocks.5.attn.proj.stats\n",
      "46 - 0.3616187572479248\n",
      "47 - transformer.blocks.5.attn.quant.stats\n",
      "47 - -0.2978476583957672\n",
      "48 - transformer.blocks.5.norm2.stats\n",
      "48 - 0.5385124683380127\n",
      "49 - transformer.blocks.5.mlp.fc1.stats\n",
      "49 - 6.217086315155029\n",
      "50 - transformer.blocks.5.mlp.fc2.stats\n",
      "50 - -0.6312984824180603\n",
      "51 - transformer.blocks.5.quant.stats\n",
      "51 - 0.2589907646179199\n",
      "52 - transformer.blocks.6.norm1.stats\n",
      "52 - -0.2521977126598358\n",
      "53 - transformer.blocks.6.attn.qkv.stats\n",
      "53 - 4.207430362701416\n",
      "54 - transformer.blocks.6.attn.proj.stats\n",
      "54 - 0.7157046794891357\n",
      "55 - transformer.blocks.6.attn.quant.stats\n",
      "55 - -0.23155467212200165\n",
      "56 - transformer.blocks.6.norm2.stats\n",
      "56 - 0.46563073992729187\n",
      "57 - transformer.blocks.6.mlp.fc1.stats\n",
      "57 - 6.34425163269043\n",
      "58 - transformer.blocks.6.mlp.fc2.stats\n",
      "58 - -1.662569522857666\n",
      "59 - transformer.blocks.6.quant.stats\n",
      "59 - 0.2747231721878052\n",
      "60 - transformer.blocks.7.norm1.stats\n",
      "60 - -0.8429831862449646\n",
      "61 - transformer.blocks.7.attn.qkv.stats\n",
      "61 - 7.404764175415039\n",
      "62 - transformer.blocks.7.attn.proj.stats\n",
      "62 - 0.7450764775276184\n",
      "63 - transformer.blocks.7.attn.quant.stats\n",
      "63 - -0.8086686134338379\n",
      "64 - transformer.blocks.7.norm2.stats\n",
      "64 - -0.23096609115600586\n",
      "65 - transformer.blocks.7.mlp.fc1.stats\n",
      "65 - 5.79833459854126\n",
      "66 - transformer.blocks.7.mlp.fc2.stats\n",
      "66 - -0.6630298495292664\n",
      "67 - transformer.blocks.7.quant.stats\n",
      "67 - -1.1866027116775513\n",
      "68 - transformer.blocks.8.norm1.stats\n",
      "68 - -1.2963876724243164\n",
      "69 - transformer.blocks.8.attn.qkv.stats\n",
      "69 - 8.663394927978516\n",
      "70 - transformer.blocks.8.attn.proj.stats\n",
      "70 - -0.2093534767627716\n",
      "71 - transformer.blocks.8.attn.quant.stats\n",
      "71 - -1.3090105056762695\n",
      "72 - transformer.blocks.8.norm2.stats\n",
      "72 - -1.478162407875061\n",
      "73 - transformer.blocks.8.mlp.fc1.stats\n",
      "73 - 4.582387447357178\n",
      "74 - transformer.blocks.8.mlp.fc2.stats\n",
      "74 - -0.6901481747627258\n",
      "75 - transformer.blocks.8.quant.stats\n",
      "75 - -0.9233713150024414\n",
      "76 - transformer.blocks.9.norm1.stats\n",
      "76 - -0.5273109078407288\n",
      "77 - transformer.blocks.9.attn.qkv.stats\n",
      "77 - 9.732210159301758\n",
      "78 - transformer.blocks.9.attn.proj.stats\n",
      "78 - -0.5554275512695312\n",
      "79 - transformer.blocks.9.attn.quant.stats\n",
      "79 - -0.5383126735687256\n",
      "80 - transformer.blocks.9.norm2.stats\n",
      "80 - -0.660555362701416\n",
      "81 - transformer.blocks.9.mlp.fc1.stats\n",
      "81 - 4.787884712219238\n",
      "82 - transformer.blocks.9.mlp.fc2.stats\n",
      "82 - -1.2896356582641602\n",
      "83 - transformer.blocks.9.quant.stats\n",
      "83 - -0.7484105229377747\n",
      "84 - transformer.blocks.10.norm1.stats\n",
      "84 - -0.32576265931129456\n",
      "85 - transformer.blocks.10.attn.qkv.stats\n",
      "85 - 8.17195987701416\n",
      "86 - transformer.blocks.10.attn.proj.stats\n",
      "86 - -0.16930274665355682\n",
      "87 - transformer.blocks.10.attn.quant.stats\n",
      "87 - -0.32643193006515503\n",
      "88 - transformer.blocks.10.norm2.stats\n",
      "88 - -0.06894318759441376\n",
      "89 - transformer.blocks.10.mlp.fc1.stats\n",
      "89 - 4.8374528884887695\n",
      "90 - transformer.blocks.10.mlp.fc2.stats\n",
      "90 - -0.07392928749322891\n",
      "91 - transformer.blocks.10.quant.stats\n",
      "91 - -0.6190575361251831\n",
      "92 - transformer.blocks.11.norm1.stats\n",
      "92 - -1.2524099349975586\n",
      "93 - transformer.blocks.11.attn.qkv.stats\n",
      "93 - 7.4620585441589355\n",
      "94 - transformer.blocks.11.attn.proj.stats\n",
      "94 - -2.003283739089966\n",
      "95 - transformer.blocks.11.attn.quant.stats\n",
      "95 - -1.2774906158447266\n",
      "96 - transformer.blocks.11.norm2.stats\n",
      "96 - 0.518545925617218\n",
      "97 - transformer.blocks.11.mlp.fc1.stats\n",
      "97 - 3.7585620880126953\n",
      "98 - transformer.blocks.11.mlp.fc2.stats\n",
      "98 - 0.4247480034828186\n",
      "99 - transformer.blocks.11.quant.stats\n",
      "99 - -0.47319644689559937\n",
      "100 - transformer.norm.stats\n",
      "100 - -0.7403141856193542\n",
      "101 - pooler.dense.stats\n",
      "101 - -1.3613133430480957\n",
      "102 - pooler.quant.stats\n",
      "102 - -1.4009816646575928\n",
      "103 - nlvr2_classifier.0.stats\n",
      "103 - -0.8297380805015564\n",
      "104 - nlvr2_classifier.1.stats\n",
      "104 - -4.106420993804932\n",
      "105 - nlvr2_classifier.3.stats\n",
      "105 - -1.5983964204788208\n",
      "106 - quant.stats\n",
      "106 - 0.6757646203041077\n",
      "Total error: 210.36483764648438\n"
     ]
    }
   ],
   "source": [
    "total_err = 0.0\n",
    "for idx, key in enumerate(act_compare_dict_static):\n",
    "    err = compute_error(act_compare_dict_static[key]['float'][0], act_compare_dict_static[key]['quantized'][0].dequantize())\n",
    "    total_err += err\n",
    "    print(f\"{idx} - {key}\")\n",
    "    print(f\"{idx} - {err}\")\n",
    "\n",
    "print(f\"Total error: {total_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# q = wt_compare_dict[key]['quantized'].flatten().dequantize()\n",
    "# f = wt_compare_dict[key]['float'].flatten()\n",
    "\n",
    "# plt.hist(q, bins=100, alpha=0.5, label='Quantized')\n",
    "# plt.hist(f, bins=100, alpha=0.5, label='Floating Point')\n",
    "\n",
    "\n",
    "# plt.title(f\"Model Weights of {key}\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys of wt_compare_dict:\n",
      "dict_keys(['text_embeddings.LayerNorm.weight', 'transformer.patch_embed.proj.weight', 'transformer.blocks.0.norm1.weight', 'transformer.blocks.0.attn.qkv._packed_params._packed_params', 'transformer.blocks.0.attn.proj._packed_params._packed_params', 'transformer.blocks.0.norm2.weight', 'transformer.blocks.0.mlp.fc1._packed_params._packed_params', 'transformer.blocks.0.mlp.fc2._packed_params._packed_params', 'transformer.blocks.1.norm1.weight', 'transformer.blocks.1.attn.qkv._packed_params._packed_params', 'transformer.blocks.1.attn.proj._packed_params._packed_params', 'transformer.blocks.1.norm2.weight', 'transformer.blocks.1.mlp.fc1._packed_params._packed_params', 'transformer.blocks.1.mlp.fc2._packed_params._packed_params', 'transformer.blocks.2.norm1.weight', 'transformer.blocks.2.attn.qkv._packed_params._packed_params', 'transformer.blocks.2.attn.proj._packed_params._packed_params', 'transformer.blocks.2.norm2.weight', 'transformer.blocks.2.mlp.fc1._packed_params._packed_params', 'transformer.blocks.2.mlp.fc2._packed_params._packed_params', 'transformer.blocks.3.norm1.weight', 'transformer.blocks.3.attn.qkv._packed_params._packed_params', 'transformer.blocks.3.attn.proj._packed_params._packed_params', 'transformer.blocks.3.norm2.weight', 'transformer.blocks.3.mlp.fc1._packed_params._packed_params', 'transformer.blocks.3.mlp.fc2._packed_params._packed_params', 'transformer.blocks.4.norm1.weight', 'transformer.blocks.4.attn.qkv._packed_params._packed_params', 'transformer.blocks.4.attn.proj._packed_params._packed_params', 'transformer.blocks.4.norm2.weight', 'transformer.blocks.4.mlp.fc1._packed_params._packed_params', 'transformer.blocks.4.mlp.fc2._packed_params._packed_params', 'transformer.blocks.5.norm1.weight', 'transformer.blocks.5.attn.qkv._packed_params._packed_params', 'transformer.blocks.5.attn.proj._packed_params._packed_params', 'transformer.blocks.5.norm2.weight', 'transformer.blocks.5.mlp.fc1._packed_params._packed_params', 'transformer.blocks.5.mlp.fc2._packed_params._packed_params', 'transformer.blocks.6.norm1.weight', 'transformer.blocks.6.attn.qkv._packed_params._packed_params', 'transformer.blocks.6.attn.proj._packed_params._packed_params', 'transformer.blocks.6.norm2.weight', 'transformer.blocks.6.mlp.fc1._packed_params._packed_params', 'transformer.blocks.6.mlp.fc2._packed_params._packed_params', 'transformer.blocks.7.norm1.weight', 'transformer.blocks.7.attn.qkv._packed_params._packed_params', 'transformer.blocks.7.attn.proj._packed_params._packed_params', 'transformer.blocks.7.norm2.weight', 'transformer.blocks.7.mlp.fc1._packed_params._packed_params', 'transformer.blocks.7.mlp.fc2._packed_params._packed_params', 'transformer.blocks.8.norm1.weight', 'transformer.blocks.8.attn.qkv._packed_params._packed_params', 'transformer.blocks.8.attn.proj._packed_params._packed_params', 'transformer.blocks.8.norm2.weight', 'transformer.blocks.8.mlp.fc1._packed_params._packed_params', 'transformer.blocks.8.mlp.fc2._packed_params._packed_params', 'transformer.blocks.9.norm1.weight', 'transformer.blocks.9.attn.qkv._packed_params._packed_params', 'transformer.blocks.9.attn.proj._packed_params._packed_params', 'transformer.blocks.9.norm2.weight', 'transformer.blocks.9.mlp.fc1._packed_params._packed_params', 'transformer.blocks.9.mlp.fc2._packed_params._packed_params', 'transformer.blocks.10.norm1.weight', 'transformer.blocks.10.attn.qkv._packed_params._packed_params', 'transformer.blocks.10.attn.proj._packed_params._packed_params', 'transformer.blocks.10.norm2.weight', 'transformer.blocks.10.mlp.fc1._packed_params._packed_params', 'transformer.blocks.10.mlp.fc2._packed_params._packed_params', 'transformer.blocks.11.norm1.weight', 'transformer.blocks.11.attn.qkv._packed_params._packed_params', 'transformer.blocks.11.attn.proj._packed_params._packed_params', 'transformer.blocks.11.norm2.weight', 'transformer.blocks.11.mlp.fc1._packed_params._packed_params', 'transformer.blocks.11.mlp.fc2._packed_params._packed_params', 'transformer.norm.weight', 'pooler.dense._packed_params._packed_params', 'vqa_classifier.0._packed_params._packed_params', 'vqa_classifier.1.weight', 'vqa_classifier.3._packed_params._packed_params'])\n",
      "text_embeddings.LayerNorm.weight tensor(inf)\n",
      "transformer.patch_embed.proj.weight tensor(inf)\n",
      "transformer.blocks.0.norm1.weight tensor(inf)\n",
      "transformer.blocks.0.attn.qkv._packed_params._packed_params tensor(27.7242)\n",
      "transformer.blocks.0.attn.proj._packed_params._packed_params tensor(22.1109)\n",
      "transformer.blocks.0.norm2.weight tensor(inf)\n",
      "transformer.blocks.0.mlp.fc1._packed_params._packed_params tensor(26.7395)\n",
      "transformer.blocks.0.mlp.fc2._packed_params._packed_params tensor(18.2086)\n",
      "transformer.blocks.1.norm1.weight tensor(inf)\n",
      "transformer.blocks.1.attn.qkv._packed_params._packed_params tensor(32.2398)\n",
      "transformer.blocks.1.attn.proj._packed_params._packed_params tensor(29.1788)\n",
      "transformer.blocks.1.norm2.weight tensor(inf)\n",
      "transformer.blocks.1.mlp.fc1._packed_params._packed_params tensor(27.6561)\n",
      "transformer.blocks.1.mlp.fc2._packed_params._packed_params tensor(16.8153)\n",
      "transformer.blocks.2.norm1.weight tensor(inf)\n",
      "transformer.blocks.2.attn.qkv._packed_params._packed_params tensor(34.0310)\n",
      "transformer.blocks.2.attn.proj._packed_params._packed_params tensor(36.2942)\n",
      "transformer.blocks.2.norm2.weight tensor(inf)\n",
      "transformer.blocks.2.mlp.fc1._packed_params._packed_params tensor(32.6989)\n",
      "transformer.blocks.2.mlp.fc2._packed_params._packed_params tensor(16.3250)\n",
      "transformer.blocks.3.norm1.weight tensor(inf)\n",
      "transformer.blocks.3.attn.qkv._packed_params._packed_params tensor(33.3137)\n",
      "transformer.blocks.3.attn.proj._packed_params._packed_params tensor(37.0851)\n",
      "transformer.blocks.3.norm2.weight tensor(inf)\n",
      "transformer.blocks.3.mlp.fc1._packed_params._packed_params tensor(32.0169)\n",
      "transformer.blocks.3.mlp.fc2._packed_params._packed_params tensor(30.6510)\n",
      "transformer.blocks.4.norm1.weight tensor(inf)\n",
      "transformer.blocks.4.attn.qkv._packed_params._packed_params tensor(34.6631)\n",
      "transformer.blocks.4.attn.proj._packed_params._packed_params tensor(38.8694)\n",
      "transformer.blocks.4.norm2.weight tensor(inf)\n",
      "transformer.blocks.4.mlp.fc1._packed_params._packed_params tensor(31.6000)\n",
      "transformer.blocks.4.mlp.fc2._packed_params._packed_params tensor(27.1258)\n",
      "transformer.blocks.5.norm1.weight tensor(inf)\n",
      "transformer.blocks.5.attn.qkv._packed_params._packed_params tensor(33.8460)\n",
      "transformer.blocks.5.attn.proj._packed_params._packed_params tensor(38.2433)\n",
      "transformer.blocks.5.norm2.weight tensor(inf)\n",
      "transformer.blocks.5.mlp.fc1._packed_params._packed_params tensor(35.5632)\n",
      "transformer.blocks.5.mlp.fc2._packed_params._packed_params tensor(29.8929)\n",
      "transformer.blocks.6.norm1.weight tensor(inf)\n",
      "transformer.blocks.6.attn.qkv._packed_params._packed_params tensor(35.7548)\n",
      "transformer.blocks.6.attn.proj._packed_params._packed_params tensor(33.8132)\n",
      "transformer.blocks.6.norm2.weight tensor(inf)\n",
      "transformer.blocks.6.mlp.fc1._packed_params._packed_params tensor(31.8049)\n",
      "transformer.blocks.6.mlp.fc2._packed_params._packed_params tensor(16.1195)\n",
      "transformer.blocks.7.norm1.weight tensor(inf)\n",
      "transformer.blocks.7.attn.qkv._packed_params._packed_params tensor(33.8472)\n",
      "transformer.blocks.7.attn.proj._packed_params._packed_params tensor(37.7150)\n",
      "transformer.blocks.7.norm2.weight tensor(inf)\n",
      "transformer.blocks.7.mlp.fc1._packed_params._packed_params tensor(23.1869)\n",
      "transformer.blocks.7.mlp.fc2._packed_params._packed_params tensor(15.1292)\n",
      "transformer.blocks.8.norm1.weight tensor(inf)\n",
      "transformer.blocks.8.attn.qkv._packed_params._packed_params tensor(35.3060)\n",
      "transformer.blocks.8.attn.proj._packed_params._packed_params tensor(36.1323)\n",
      "transformer.blocks.8.norm2.weight tensor(inf)\n",
      "transformer.blocks.8.mlp.fc1._packed_params._packed_params tensor(18.8654)\n",
      "transformer.blocks.8.mlp.fc2._packed_params._packed_params tensor(18.3614)\n",
      "transformer.blocks.9.norm1.weight tensor(inf)\n",
      "transformer.blocks.9.attn.qkv._packed_params._packed_params tensor(33.0471)\n",
      "transformer.blocks.9.attn.proj._packed_params._packed_params tensor(30.6102)\n",
      "transformer.blocks.9.norm2.weight tensor(inf)\n",
      "transformer.blocks.9.mlp.fc1._packed_params._packed_params tensor(22.5741)\n",
      "transformer.blocks.9.mlp.fc2._packed_params._packed_params tensor(21.0376)\n",
      "transformer.blocks.10.norm1.weight tensor(inf)\n",
      "transformer.blocks.10.attn.qkv._packed_params._packed_params tensor(30.8587)\n",
      "transformer.blocks.10.attn.proj._packed_params._packed_params tensor(34.4623)\n",
      "transformer.blocks.10.norm2.weight tensor(inf)\n",
      "transformer.blocks.10.mlp.fc1._packed_params._packed_params tensor(33.3507)\n",
      "transformer.blocks.10.mlp.fc2._packed_params._packed_params tensor(29.1932)\n",
      "transformer.blocks.11.norm1.weight tensor(inf)\n",
      "transformer.blocks.11.attn.qkv._packed_params._packed_params tensor(31.0226)\n",
      "transformer.blocks.11.attn.proj._packed_params._packed_params tensor(26.6492)\n",
      "transformer.blocks.11.norm2.weight tensor(inf)\n",
      "transformer.blocks.11.mlp.fc1._packed_params._packed_params tensor(29.1284)\n",
      "transformer.blocks.11.mlp.fc2._packed_params._packed_params tensor(19.5302)\n",
      "transformer.norm.weight tensor(inf)\n",
      "pooler.dense._packed_params._packed_params tensor(36.7458)\n",
      "vqa_classifier.0._packed_params._packed_params tensor(37.6585)\n",
      "vqa_classifier.1.weight tensor(inf)\n",
      "vqa_classifier.3._packed_params._packed_params tensor(27.2145)\n"
     ]
    }
   ],
   "source": [
    "# ======== Dynamic quantization comparison ========\n",
    "wt_compare_dict_dynamic = ns.compare_weights(model.state_dict(), custom_8bit.state_dict())\n",
    "\n",
    "print('keys of wt_compare_dict:')\n",
    "print(wt_compare_dict_dynamic.keys())\n",
    "\n",
    "# key = 'text_embeddings.LayerNorm.weight'\n",
    "\n",
    "# print(f\"\\nkeys of wt_compare_dict entry for {key} weight:\")\n",
    "# print(wt_compare_dict_dynamic[key].keys())\n",
    "# print(wt_compare_dict_dynamic[key]['float'].shape)\n",
    "# print(wt_compare_dict_dynamic[key]['quantized'].shape)\n",
    "\n",
    "for key in wt_compare_dict_dynamic:\n",
    "    if wt_compare_dict_dynamic[key]['quantized'].is_quantized:\n",
    "        print(key, compute_error(wt_compare_dict_dynamic[key]['float'], wt_compare_dict_dynamic[key]['quantized'].dequantize()))\n",
    "    else:\n",
    "        print(key, compute_error(wt_compare_dict_dynamic[key]['float'], wt_compare_dict_dynamic[key]['quantized']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# q = wt_compare_dict[key]['quantized'].flatten().dequantize()\n",
    "# f = wt_compare_dict[key]['float'].flatten()\n",
    "\n",
    "# plt.hist(q, bins=100, alpha=0.5, label='Quantized')\n",
    "# plt.hist(f, bins=100, alpha=0.5, label='Floating Point')\n",
    "\n",
    "\n",
    "# plt.title(f\"Model Weights of {key}\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text_embeddings.LayerNorm.stats', 'text_embeddings.quant.stats', 'transformer.patch_embed.proj.stats', 'transformer.patch_embed.quant.stats', 'transformer.blocks.0.norm1.stats', 'transformer.blocks.0.attn.qkv.stats', 'transformer.blocks.0.attn.proj.stats', 'transformer.blocks.0.attn.quant.stats', 'transformer.blocks.0.norm2.stats', 'transformer.blocks.0.mlp.fc1.stats', 'transformer.blocks.0.mlp.fc2.stats', 'transformer.blocks.0.quant.stats', 'transformer.blocks.1.norm1.stats', 'transformer.blocks.1.attn.qkv.stats', 'transformer.blocks.1.attn.proj.stats', 'transformer.blocks.1.attn.quant.stats', 'transformer.blocks.1.norm2.stats', 'transformer.blocks.1.mlp.fc1.stats', 'transformer.blocks.1.mlp.fc2.stats', 'transformer.blocks.1.quant.stats', 'transformer.blocks.2.norm1.stats', 'transformer.blocks.2.attn.qkv.stats', 'transformer.blocks.2.attn.proj.stats', 'transformer.blocks.2.attn.quant.stats', 'transformer.blocks.2.norm2.stats', 'transformer.blocks.2.mlp.fc1.stats', 'transformer.blocks.2.mlp.fc2.stats', 'transformer.blocks.2.quant.stats', 'transformer.blocks.3.norm1.stats', 'transformer.blocks.3.attn.qkv.stats', 'transformer.blocks.3.attn.proj.stats', 'transformer.blocks.3.attn.quant.stats', 'transformer.blocks.3.norm2.stats', 'transformer.blocks.3.mlp.fc1.stats', 'transformer.blocks.3.mlp.fc2.stats', 'transformer.blocks.3.quant.stats', 'transformer.blocks.4.norm1.stats', 'transformer.blocks.4.attn.qkv.stats', 'transformer.blocks.4.attn.proj.stats', 'transformer.blocks.4.attn.quant.stats', 'transformer.blocks.4.norm2.stats', 'transformer.blocks.4.mlp.fc1.stats', 'transformer.blocks.4.mlp.fc2.stats', 'transformer.blocks.4.quant.stats', 'transformer.blocks.5.norm1.stats', 'transformer.blocks.5.attn.qkv.stats', 'transformer.blocks.5.attn.proj.stats', 'transformer.blocks.5.attn.quant.stats', 'transformer.blocks.5.norm2.stats', 'transformer.blocks.5.mlp.fc1.stats', 'transformer.blocks.5.mlp.fc2.stats', 'transformer.blocks.5.quant.stats', 'transformer.blocks.6.norm1.stats', 'transformer.blocks.6.attn.qkv.stats', 'transformer.blocks.6.attn.proj.stats', 'transformer.blocks.6.attn.quant.stats', 'transformer.blocks.6.norm2.stats', 'transformer.blocks.6.mlp.fc1.stats', 'transformer.blocks.6.mlp.fc2.stats', 'transformer.blocks.6.quant.stats', 'transformer.blocks.7.norm1.stats', 'transformer.blocks.7.attn.qkv.stats', 'transformer.blocks.7.attn.proj.stats', 'transformer.blocks.7.attn.quant.stats', 'transformer.blocks.7.norm2.stats', 'transformer.blocks.7.mlp.fc1.stats', 'transformer.blocks.7.mlp.fc2.stats', 'transformer.blocks.7.quant.stats', 'transformer.blocks.8.norm1.stats', 'transformer.blocks.8.attn.qkv.stats', 'transformer.blocks.8.attn.proj.stats', 'transformer.blocks.8.attn.quant.stats', 'transformer.blocks.8.norm2.stats', 'transformer.blocks.8.mlp.fc1.stats', 'transformer.blocks.8.mlp.fc2.stats', 'transformer.blocks.8.quant.stats', 'transformer.blocks.9.norm1.stats', 'transformer.blocks.9.attn.qkv.stats', 'transformer.blocks.9.attn.proj.stats', 'transformer.blocks.9.attn.quant.stats', 'transformer.blocks.9.norm2.stats', 'transformer.blocks.9.mlp.fc1.stats', 'transformer.blocks.9.mlp.fc2.stats', 'transformer.blocks.9.quant.stats', 'transformer.blocks.10.norm1.stats', 'transformer.blocks.10.attn.qkv.stats', 'transformer.blocks.10.attn.proj.stats', 'transformer.blocks.10.attn.quant.stats', 'transformer.blocks.10.norm2.stats', 'transformer.blocks.10.mlp.fc1.stats', 'transformer.blocks.10.mlp.fc2.stats', 'transformer.blocks.10.quant.stats', 'transformer.blocks.11.norm1.stats', 'transformer.blocks.11.attn.qkv.stats', 'transformer.blocks.11.attn.proj.stats', 'transformer.blocks.11.attn.quant.stats', 'transformer.blocks.11.norm2.stats', 'transformer.blocks.11.mlp.fc1.stats', 'transformer.blocks.11.mlp.fc2.stats', 'transformer.blocks.11.quant.stats', 'transformer.norm.stats', 'pooler.dense.stats', 'pooler.quant.stats', 'nlvr2_classifier.0.stats', 'nlvr2_classifier.1.stats', 'nlvr2_classifier.3.stats', 'quant.stats'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/pytorch_lightning/core/module.py:445: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    }
   ],
   "source": [
    "# act_compare_dict_dynamic = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(model_dynamic), full_batch)\n",
    "act_compare_dict_dynamic = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(custom_8bit), infer_dm)\n",
    "print(act_compare_dict_dynamic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - text_embeddings.LayerNorm.stats\n",
      "0 - 38.937477111816406\n",
      "1 - text_embeddings.quant.stats\n",
      "1 - 38.74382019042969\n",
      "2 - transformer.patch_embed.proj.stats\n",
      "2 - inf\n",
      "3 - transformer.patch_embed.quant.stats\n",
      "3 - inf\n",
      "4 - transformer.blocks.0.norm1.stats\n",
      "4 - -0.9947296977043152\n",
      "5 - transformer.blocks.0.attn.qkv.stats\n",
      "5 - 4.067282676696777\n",
      "6 - transformer.blocks.0.attn.proj.stats\n",
      "6 - 0.0086312685161829\n",
      "7 - transformer.blocks.0.attn.quant.stats\n",
      "7 - -0.9947296977043152\n",
      "8 - transformer.blocks.0.norm2.stats\n",
      "8 - -0.7651967406272888\n",
      "9 - transformer.blocks.0.mlp.fc1.stats\n",
      "9 - 3.797595262527466\n",
      "10 - transformer.blocks.0.mlp.fc2.stats\n",
      "10 - -2.599177360534668\n",
      "11 - transformer.blocks.0.quant.stats\n",
      "11 - -1.2061665058135986\n",
      "12 - transformer.blocks.1.norm1.stats\n",
      "12 - -1.2639917135238647\n",
      "13 - transformer.blocks.1.attn.qkv.stats\n",
      "13 - 3.756920099258423\n",
      "14 - transformer.blocks.1.attn.proj.stats\n",
      "14 - -0.4064253270626068\n",
      "15 - transformer.blocks.1.attn.quant.stats\n",
      "15 - -1.2639917135238647\n",
      "16 - transformer.blocks.1.norm2.stats\n",
      "16 - -0.4531663656234741\n",
      "17 - transformer.blocks.1.mlp.fc1.stats\n",
      "17 - 2.995737075805664\n",
      "18 - transformer.blocks.1.mlp.fc2.stats\n",
      "18 - -2.3605735301971436\n",
      "19 - transformer.blocks.1.quant.stats\n",
      "19 - -0.9375766515731812\n",
      "20 - transformer.blocks.2.norm1.stats\n",
      "20 - -1.4156384468078613\n",
      "21 - transformer.blocks.2.attn.qkv.stats\n",
      "21 - 0.2055465281009674\n",
      "22 - transformer.blocks.2.attn.proj.stats\n",
      "22 - -1.4463204145431519\n",
      "23 - transformer.blocks.2.attn.quant.stats\n",
      "23 - -1.4156384468078613\n",
      "24 - transformer.blocks.2.norm2.stats\n",
      "24 - -0.5262982249259949\n",
      "25 - transformer.blocks.2.mlp.fc1.stats\n",
      "25 - 3.6729841232299805\n",
      "26 - transformer.blocks.2.mlp.fc2.stats\n",
      "26 - -2.239218235015869\n",
      "27 - transformer.blocks.2.quant.stats\n",
      "27 - -1.0070098638534546\n",
      "28 - transformer.blocks.3.norm1.stats\n",
      "28 - -1.5181028842926025\n",
      "29 - transformer.blocks.3.attn.qkv.stats\n",
      "29 - 0.4622040390968323\n",
      "30 - transformer.blocks.3.attn.proj.stats\n",
      "30 - -1.0830680131912231\n",
      "31 - transformer.blocks.3.attn.quant.stats\n",
      "31 - -1.5181028842926025\n",
      "32 - transformer.blocks.3.norm2.stats\n",
      "32 - -0.6207751035690308\n",
      "33 - transformer.blocks.3.mlp.fc1.stats\n",
      "33 - 3.807046413421631\n",
      "34 - transformer.blocks.3.mlp.fc2.stats\n",
      "34 - -2.3117516040802\n",
      "35 - transformer.blocks.3.quant.stats\n",
      "35 - -1.0361464023590088\n",
      "36 - transformer.blocks.4.norm1.stats\n",
      "36 - -1.5832056999206543\n",
      "37 - transformer.blocks.4.attn.qkv.stats\n",
      "37 - 0.9713652729988098\n",
      "38 - transformer.blocks.4.attn.proj.stats\n",
      "38 - -0.9972188472747803\n",
      "39 - transformer.blocks.4.attn.quant.stats\n",
      "39 - -1.5832056999206543\n",
      "40 - transformer.blocks.4.norm2.stats\n",
      "40 - -0.7022062540054321\n",
      "41 - transformer.blocks.4.mlp.fc1.stats\n",
      "41 - 4.295453071594238\n",
      "42 - transformer.blocks.4.mlp.fc2.stats\n",
      "42 - -2.0235421657562256\n",
      "43 - transformer.blocks.4.quant.stats\n",
      "43 - -1.099348545074463\n",
      "44 - transformer.blocks.5.norm1.stats\n",
      "44 - -1.596822738647461\n",
      "45 - transformer.blocks.5.attn.qkv.stats\n",
      "45 - 1.5097771883010864\n",
      "46 - transformer.blocks.5.attn.proj.stats\n",
      "46 - -0.9685028791427612\n",
      "47 - transformer.blocks.5.attn.quant.stats\n",
      "47 - -1.596822738647461\n",
      "48 - transformer.blocks.5.norm2.stats\n",
      "48 - -0.7794027328491211\n",
      "49 - transformer.blocks.5.mlp.fc1.stats\n",
      "49 - 4.550298690795898\n",
      "50 - transformer.blocks.5.mlp.fc2.stats\n",
      "50 - -1.8420194387435913\n",
      "51 - transformer.blocks.5.quant.stats\n",
      "51 - -1.0877264738082886\n",
      "52 - transformer.blocks.6.norm1.stats\n",
      "52 - -1.532160758972168\n",
      "53 - transformer.blocks.6.attn.qkv.stats\n",
      "53 - 2.664987087249756\n",
      "54 - transformer.blocks.6.attn.proj.stats\n",
      "54 - -0.4125916659832001\n",
      "55 - transformer.blocks.6.attn.quant.stats\n",
      "55 - -1.532160758972168\n",
      "56 - transformer.blocks.6.norm2.stats\n",
      "56 - -0.84002685546875\n",
      "57 - transformer.blocks.6.mlp.fc1.stats\n",
      "57 - 4.785156726837158\n",
      "58 - transformer.blocks.6.mlp.fc2.stats\n",
      "58 - -2.8672192096710205\n",
      "59 - transformer.blocks.6.quant.stats\n",
      "59 - -1.0149006843566895\n",
      "60 - transformer.blocks.7.norm1.stats\n",
      "60 - -1.4324822425842285\n",
      "61 - transformer.blocks.7.attn.qkv.stats\n",
      "61 - 6.356818199157715\n",
      "62 - transformer.blocks.7.attn.proj.stats\n",
      "62 - 0.10356390476226807\n",
      "63 - transformer.blocks.7.attn.quant.stats\n",
      "63 - -1.4324822425842285\n",
      "64 - transformer.blocks.7.norm2.stats\n",
      "64 - -0.7347779870033264\n",
      "65 - transformer.blocks.7.mlp.fc1.stats\n",
      "65 - 4.888020038604736\n",
      "66 - transformer.blocks.7.mlp.fc2.stats\n",
      "66 - -0.6613329648971558\n",
      "67 - transformer.blocks.7.quant.stats\n",
      "67 - -2.17708158493042\n",
      "68 - transformer.blocks.8.norm1.stats\n",
      "68 - -1.1298776865005493\n",
      "69 - transformer.blocks.8.attn.qkv.stats\n",
      "69 - 8.357536315917969\n",
      "70 - transformer.blocks.8.attn.proj.stats\n",
      "70 - 1.0622156858444214\n",
      "71 - transformer.blocks.8.attn.quant.stats\n",
      "71 - -1.1298776865005493\n",
      "72 - transformer.blocks.8.norm2.stats\n",
      "72 - -0.6356834769248962\n",
      "73 - transformer.blocks.8.mlp.fc1.stats\n",
      "73 - 4.995960712432861\n",
      "74 - transformer.blocks.8.mlp.fc2.stats\n",
      "74 - -1.1753596067428589\n",
      "75 - transformer.blocks.8.quant.stats\n",
      "75 - -1.8502707481384277\n",
      "76 - transformer.blocks.9.norm1.stats\n",
      "76 - -0.9405046701431274\n",
      "77 - transformer.blocks.9.attn.qkv.stats\n",
      "77 - 9.548083305358887\n",
      "78 - transformer.blocks.9.attn.proj.stats\n",
      "78 - 1.0669102668762207\n",
      "79 - transformer.blocks.9.attn.quant.stats\n",
      "79 - -0.9405046701431274\n",
      "80 - transformer.blocks.9.norm2.stats\n",
      "80 - -0.6734306216239929\n",
      "81 - transformer.blocks.9.mlp.fc1.stats\n",
      "81 - 6.054743766784668\n",
      "82 - transformer.blocks.9.mlp.fc2.stats\n",
      "82 - -1.8431978225708008\n",
      "83 - transformer.blocks.9.quant.stats\n",
      "83 - -1.5552462339401245\n",
      "84 - transformer.blocks.10.norm1.stats\n",
      "84 - -0.9259788990020752\n",
      "85 - transformer.blocks.10.attn.qkv.stats\n",
      "85 - 8.482502937316895\n",
      "86 - transformer.blocks.10.attn.proj.stats\n",
      "86 - 1.461094856262207\n",
      "87 - transformer.blocks.10.attn.quant.stats\n",
      "87 - -0.9259788990020752\n",
      "88 - transformer.blocks.10.norm2.stats\n",
      "88 - 0.12690572440624237\n",
      "89 - transformer.blocks.10.mlp.fc1.stats\n",
      "89 - 8.451335906982422\n",
      "90 - transformer.blocks.10.mlp.fc2.stats\n",
      "90 - 0.31764328479766846\n",
      "91 - transformer.blocks.10.quant.stats\n",
      "91 - -1.6036853790283203\n",
      "92 - transformer.blocks.11.norm1.stats\n",
      "92 - -0.2813013792037964\n",
      "93 - transformer.blocks.11.attn.qkv.stats\n",
      "93 - 9.806869506835938\n",
      "94 - transformer.blocks.11.attn.proj.stats\n",
      "94 - 8.845454216003418\n",
      "95 - transformer.blocks.11.attn.quant.stats\n",
      "95 - -0.2813013792037964\n",
      "96 - transformer.blocks.11.norm2.stats\n",
      "96 - 1.1380751132965088\n",
      "97 - transformer.blocks.11.mlp.fc1.stats\n",
      "97 - 8.49392032623291\n",
      "98 - transformer.blocks.11.mlp.fc2.stats\n",
      "98 - 2.20888614654541\n",
      "99 - transformer.blocks.11.quant.stats\n",
      "99 - -1.4265937805175781\n",
      "100 - transformer.norm.stats\n",
      "100 - 2.344665050506592\n",
      "101 - pooler.dense.stats\n",
      "101 - 7.120966911315918\n",
      "102 - pooler.quant.stats\n",
      "102 - 7.6579718589782715\n",
      "103 - nlvr2_classifier.0.stats\n",
      "103 - 13.151965141296387\n",
      "104 - nlvr2_classifier.1.stats\n",
      "104 - 13.658308982849121\n",
      "105 - nlvr2_classifier.3.stats\n",
      "105 - 15.043009757995605\n",
      "106 - quant.stats\n",
      "106 - 1.2455402612686157\n",
      "Total error: 194.0214385986328\n"
     ]
    }
   ],
   "source": [
    "total_err = 0\n",
    "for idx, key in enumerate(act_compare_dict_dynamic):\n",
    "    err = compute_error(act_compare_dict_dynamic[key]['float'][0][0], act_compare_dict_dynamic[key]['quantized'][0][0])\n",
    "    # print(type(err))\n",
    "    if torch.isinf(err):\n",
    "        pass\n",
    "    else:\n",
    "        total_err += err\n",
    "    print(f\"{idx} - {key}\")\n",
    "    print(f\"{idx} - {err}\")\n",
    "\n",
    "print(f\"Total error: {total_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViLTransformerSS(\n",
      "  (text_embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(40, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (quant): QuantStub()\n",
      "    (dequant): DeQuantStub()\n",
      "  )\n",
      "  (token_type_embeddings): Embedding(3, 768)\n",
      "  (transformer): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      "      (quant): QuantStub()\n",
      "      (dequant): DeQuantStub()\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.1, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "          (quant): QuantStub()\n",
      "          (dequant): DeQuantStub()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): QuantStub()\n",
      "        (dequant): DeQuantStub()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (pooler): Pooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "    (quant): QuantStub()\n",
      "    (dequant): DeQuantStub()\n",
      "  )\n",
      "  (nlvr2_classifier): Sequential(\n",
      "    (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): Linear(in_features=1536, out_features=2, bias=True)\n",
      "  )\n",
      "  (train_nlvr2_accuracy): Accuracy()\n",
      "  (train_nlvr2_loss): Scalar()\n",
      "  (dev_nlvr2_accuracy): Accuracy()\n",
      "  (dev_nlvr2_loss): Scalar()\n",
      "  (test_nlvr2_accuracy): Accuracy()\n",
      "  (test_nlvr2_loss): Scalar()\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViLTransformerSS(\n",
      "  (text_embeddings): BertEmbeddings(\n",
      "    (word_embeddings): QuantizedEmbedding(num_embeddings=30522, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
      "    (position_embeddings): QuantizedEmbedding(num_embeddings=40, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
      "    (token_type_embeddings): QuantizedEmbedding(num_embeddings=2, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
      "    (LayerNorm): QuantizedLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): QuantizedDropout(p=0.1, inplace=False)\n",
      "    (quant): Quantize(scale=tensor([0.0120]), zero_point=tensor([63]), dtype=torch.quint8)\n",
      "    (dequant): DeQuantize()\n",
      "  )\n",
      "  (token_type_embeddings): QuantizedEmbedding(num_embeddings=3, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
      "  (transformer): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): QuantizedConv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), scale=0.2465784102678299, zero_point=61)\n",
      "      (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
      "      (dequant): DeQuantize()\n",
      "    )\n",
      "    (pos_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        (norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): QuantizedLinear(in_features=768, out_features=2304, scale=0.12492784112691879, zero_point=64, qscheme=torch.per_channel_affine)\n",
      "          (attn_drop): QuantizedDropout(p=0.0, inplace=False)\n",
      "          (proj): QuantizedLinear(in_features=768, out_features=768, scale=0.27863624691963196, zero_point=100, qscheme=torch.per_channel_affine)\n",
      "          (proj_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "          (quant): Quantize(scale=tensor([0.0385]), zero_point=tensor([62]), dtype=torch.quint8)\n",
      "          (dequant): DeQuantize()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.10200261324644089, zero_point=79, qscheme=torch.per_channel_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=0.5394390225410461, zero_point=106, qscheme=torch.per_channel_affine)\n",
      "          (drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): Quantize(scale=tensor([0.2708]), zero_point=tensor([61]), dtype=torch.quint8)\n",
      "        (dequant): DeQuantize()\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): QuantizedLinear(in_features=768, out_features=2304, scale=0.15472693741321564, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "          (attn_drop): QuantizedDropout(p=0.0, inplace=False)\n",
      "          (proj): QuantizedLinear(in_features=768, out_features=768, scale=0.0534946471452713, zero_point=65, qscheme=torch.per_channel_affine)\n",
      "          (proj_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "          (quant): Quantize(scale=tensor([0.0372]), zero_point=tensor([61]), dtype=torch.quint8)\n",
      "          (dequant): DeQuantize()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.09735607355833054, zero_point=96, qscheme=torch.per_channel_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=0.23655180633068085, zero_point=106, qscheme=torch.per_channel_affine)\n",
      "          (drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): Quantize(scale=tensor([0.8231]), zero_point=tensor([105]), dtype=torch.quint8)\n",
      "        (dequant): DeQuantize()\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): QuantizedLinear(in_features=768, out_features=2304, scale=0.10447195917367935, zero_point=64, qscheme=torch.per_channel_affine)\n",
      "          (attn_drop): QuantizedDropout(p=0.0, inplace=False)\n",
      "          (proj): QuantizedLinear(in_features=768, out_features=768, scale=0.05652575567364693, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "          (proj_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "          (quant): Quantize(scale=tensor([0.0421]), zero_point=tensor([63]), dtype=torch.quint8)\n",
      "          (dequant): DeQuantize()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.08510777354240417, zero_point=90, qscheme=torch.per_channel_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=0.19182977080345154, zero_point=98, qscheme=torch.per_channel_affine)\n",
      "          (drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): Quantize(scale=tensor([1.0681]), zero_point=tensor([105]), dtype=torch.quint8)\n",
      "        (dequant): DeQuantize()\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): QuantizedLinear(in_features=768, out_features=2304, scale=0.12167397886514664, zero_point=61, qscheme=torch.per_channel_affine)\n",
      "          (attn_drop): QuantizedDropout(p=0.0, inplace=False)\n",
      "          (proj): QuantizedLinear(in_features=768, out_features=768, scale=0.058553487062454224, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "          (proj_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "          (quant): Quantize(scale=tensor([0.0390]), zero_point=tensor([63]), dtype=torch.quint8)\n",
      "          (dequant): DeQuantize()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.07369934767484665, zero_point=86, qscheme=torch.per_channel_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=0.06079676002264023, zero_point=66, qscheme=torch.per_channel_affine)\n",
      "          (drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): Quantize(scale=tensor([1.2680]), zero_point=tensor([103]), dtype=torch.quint8)\n",
      "        (dequant): DeQuantize()\n",
      "      )\n",
      "      (4): Block(\n",
      "        (norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): QuantizedLinear(in_features=768, out_features=2304, scale=0.11904111504554749, zero_point=65, qscheme=torch.per_channel_affine)\n",
      "          (attn_drop): QuantizedDropout(p=0.0, inplace=False)\n",
      "          (proj): QuantizedLinear(in_features=768, out_features=768, scale=0.05614360421895981, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "          (proj_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "          (quant): Quantize(scale=tensor([0.0378]), zero_point=tensor([63]), dtype=torch.quint8)\n",
      "          (dequant): DeQuantize()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.07786823064088821, zero_point=84, qscheme=torch.per_channel_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=0.12002807855606079, zero_point=77, qscheme=torch.per_channel_affine)\n",
      "          (drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): Quantize(scale=tensor([1.2903]), zero_point=tensor([102]), dtype=torch.quint8)\n",
      "        (dequant): DeQuantize()\n",
      "      )\n",
      "      (5): Block(\n",
      "        (norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): QuantizedLinear(in_features=768, out_features=2304, scale=0.1375829130411148, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "          (attn_drop): QuantizedDropout(p=0.0, inplace=False)\n",
      "          (proj): QuantizedLinear(in_features=768, out_features=768, scale=0.07467816770076752, zero_point=59, qscheme=torch.per_channel_affine)\n",
      "          (proj_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "          (quant): Quantize(scale=tensor([0.0427]), zero_point=tensor([61]), dtype=torch.quint8)\n",
      "          (dequant): DeQuantize()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.09074079245328903, zero_point=85, qscheme=torch.per_channel_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=0.18053790926933289, zero_point=75, qscheme=torch.per_channel_affine)\n",
      "          (drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): Quantize(scale=tensor([1.3396]), zero_point=tensor([100]), dtype=torch.quint8)\n",
      "        (dequant): DeQuantize()\n",
      "      )\n",
      "      (6): Block(\n",
      "        (norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): QuantizedLinear(in_features=768, out_features=2304, scale=0.15529251098632812, zero_point=64, qscheme=torch.per_channel_affine)\n",
      "          (attn_drop): QuantizedDropout(p=0.0, inplace=False)\n",
      "          (proj): QuantizedLinear(in_features=768, out_features=768, scale=0.13922618329524994, zero_point=58, qscheme=torch.per_channel_affine)\n",
      "          (proj_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "          (quant): Quantize(scale=tensor([0.0456]), zero_point=tensor([61]), dtype=torch.quint8)\n",
      "          (dequant): DeQuantize()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.22372540831565857, zero_point=72, qscheme=torch.per_channel_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=5.199750900268555, zero_point=80, qscheme=torch.per_channel_affine)\n",
      "          (drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): Quantize(scale=tensor([1.3721]), zero_point=tensor([99]), dtype=torch.quint8)\n",
      "        (dequant): DeQuantize()\n",
      "      )\n",
      "      (7): Block(\n",
      "        (norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): QuantizedLinear(in_features=768, out_features=2304, scale=0.1954934298992157, zero_point=67, qscheme=torch.per_channel_affine)\n",
      "          (attn_drop): QuantizedDropout(p=0.0, inplace=False)\n",
      "          (proj): QuantizedLinear(in_features=768, out_features=768, scale=0.23527701199054718, zero_point=65, qscheme=torch.per_channel_affine)\n",
      "          (proj_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "          (quant): Quantize(scale=tensor([0.0554]), zero_point=tensor([66]), dtype=torch.quint8)\n",
      "          (dequant): DeQuantize()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.651413083076477, zero_point=74, qscheme=torch.per_channel_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=12.620697021484375, zero_point=78, qscheme=torch.per_channel_affine)\n",
      "          (drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): Quantize(scale=tensor([5.1881]), zero_point=tensor([83]), dtype=torch.quint8)\n",
      "        (dequant): DeQuantize()\n",
      "      )\n",
      "      (8): Block(\n",
      "        (norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): QuantizedLinear(in_features=768, out_features=2304, scale=0.22603781521320343, zero_point=62, qscheme=torch.per_channel_affine)\n",
      "          (attn_drop): QuantizedDropout(p=0.0, inplace=False)\n",
      "          (proj): QuantizedLinear(in_features=768, out_features=768, scale=0.1469428390264511, zero_point=61, qscheme=torch.per_channel_affine)\n",
      "          (proj_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "          (quant): Quantize(scale=tensor([0.0527]), zero_point=tensor([62]), dtype=torch.quint8)\n",
      "          (dequant): DeQuantize()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.6497867107391357, zero_point=82, qscheme=torch.per_channel_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=8.821465492248535, zero_point=81, qscheme=torch.per_channel_affine)\n",
      "          (drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): Quantize(scale=tensor([15.2433]), zero_point=tensor([79]), dtype=torch.quint8)\n",
      "        (dequant): DeQuantize()\n",
      "      )\n",
      "      (9): Block(\n",
      "        (norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): QuantizedLinear(in_features=768, out_features=2304, scale=0.2295437902212143, zero_point=62, qscheme=torch.per_channel_affine)\n",
      "          (attn_drop): QuantizedDropout(p=0.0, inplace=False)\n",
      "          (proj): QuantizedLinear(in_features=768, out_features=768, scale=0.2954007387161255, zero_point=62, qscheme=torch.per_channel_affine)\n",
      "          (proj_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "          (quant): Quantize(scale=tensor([0.0707]), zero_point=tensor([65]), dtype=torch.quint8)\n",
      "          (dequant): DeQuantize()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.3547573983669281, zero_point=81, qscheme=torch.per_channel_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=4.396595478057861, zero_point=67, qscheme=torch.per_channel_affine)\n",
      "          (drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): Quantize(scale=tensor([27.0621]), zero_point=tensor([69]), dtype=torch.quint8)\n",
      "        (dequant): DeQuantize()\n",
      "      )\n",
      "      (10): Block(\n",
      "        (norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): QuantizedLinear(in_features=768, out_features=2304, scale=0.28237566351890564, zero_point=62, qscheme=torch.per_channel_affine)\n",
      "          (attn_drop): QuantizedDropout(p=0.0, inplace=False)\n",
      "          (proj): QuantizedLinear(in_features=768, out_features=768, scale=0.32072168588638306, zero_point=72, qscheme=torch.per_channel_affine)\n",
      "          (proj_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "          (quant): Quantize(scale=tensor([0.0888]), zero_point=tensor([59]), dtype=torch.quint8)\n",
      "          (dequant): DeQuantize()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.4795296788215637, zero_point=89, qscheme=torch.per_channel_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=1.0709540843963623, zero_point=53, qscheme=torch.per_channel_affine)\n",
      "          (drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): Quantize(scale=tensor([31.4832]), zero_point=tensor([68]), dtype=torch.quint8)\n",
      "        (dequant): DeQuantize()\n",
      "      )\n",
      "      (11): Block(\n",
      "        (norm1): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): QuantizedLinear(in_features=768, out_features=2304, scale=0.25789180397987366, zero_point=67, qscheme=torch.per_channel_affine)\n",
      "          (attn_drop): QuantizedDropout(p=0.0, inplace=False)\n",
      "          (proj): QuantizedLinear(in_features=768, out_features=768, scale=4.655595302581787, zero_point=83, qscheme=torch.per_channel_affine)\n",
      "          (proj_drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "          (quant): Quantize(scale=tensor([0.1539]), zero_point=tensor([60]), dtype=torch.quint8)\n",
      "          (dequant): DeQuantize()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedLinear(in_features=768, out_features=3072, scale=0.6355603337287903, zero_point=50, qscheme=torch.per_channel_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): QuantizedLinear(in_features=3072, out_features=768, scale=14.024627685546875, zero_point=82, qscheme=torch.per_channel_affine)\n",
      "          (drop): QuantizedDropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): Quantize(scale=tensor([32.5525]), zero_point=tensor([66]), dtype=torch.quint8)\n",
      "        (dequant): DeQuantize()\n",
      "      )\n",
      "    )\n",
      "    (norm): QuantizedLayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (pooler): Pooler(\n",
      "    (dense): QuantizedLinear(in_features=768, out_features=768, scale=0.07670333236455917, zero_point=61, qscheme=torch.per_channel_affine)\n",
      "    (activation): Tanh()\n",
      "    (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
      "    (dequant): DeQuantize()\n",
      "  )\n",
      "  (nlvr2_classifier): Sequential(\n",
      "    (0): QuantizedLinear(in_features=1536, out_features=1536, scale=0.07336397469043732, zero_point=52, qscheme=torch.per_channel_affine)\n",
      "    (1): QuantizedLayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): QuantizedLinear(in_features=1536, out_features=2, scale=0.069314144551754, zero_point=53, qscheme=torch.per_channel_affine)\n",
      "  )\n",
      "  (train_nlvr2_accuracy): Accuracy()\n",
      "  (train_nlvr2_loss): Scalar()\n",
      "  (dev_nlvr2_accuracy): Accuracy()\n",
      "  (dev_nlvr2_loss): Scalar()\n",
      "  (test_nlvr2_accuracy): Accuracy()\n",
      "  (test_nlvr2_loss): Scalar()\n",
      "  (quant): Quantize(scale=tensor([32.1737]), zero_point=tensor([66]), dtype=torch.quint8)\n",
      "  (dequant): DeQuantize()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViLTransformerSS(\n",
      "  (text_embeddings): BertEmbeddings(\n",
      "    (word_embeddings): QuantizedEmbedding(num_embeddings=30522, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
      "    (position_embeddings): QuantizedEmbedding(num_embeddings=40, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
      "    (token_type_embeddings): QuantizedEmbedding(num_embeddings=2, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (quant): QuantStub()\n",
      "    (dequant): DeQuantStub()\n",
      "  )\n",
      "  (token_type_embeddings): QuantizedEmbedding(num_embeddings=3, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
      "  (transformer): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      "      (quant): QuantStub()\n",
      "      (dequant): DeQuantStub()\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.1, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): DynamicQuantizedLinear(in_features=768, out_features=2304, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "          (quant): QuantStub()\n",
      "          (dequant): DeQuantStub()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (quant): QuantStub()\n",
      "        (dequant): DeQuantStub()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (pooler): Pooler(\n",
      "    (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "    (activation): Tanh()\n",
      "    (quant): QuantStub()\n",
      "    (dequant): DeQuantStub()\n",
      "  )\n",
      "  (nlvr2_classifier): Sequential(\n",
      "    (0): DynamicQuantizedLinear(in_features=1536, out_features=1536, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): DynamicQuantizedLinear(in_features=1536, out_features=2, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "  )\n",
      "  (train_nlvr2_accuracy): Accuracy()\n",
      "  (train_nlvr2_loss): Scalar()\n",
      "  (dev_nlvr2_accuracy): Accuracy()\n",
      "  (dev_nlvr2_loss): Scalar()\n",
      "  (test_nlvr2_accuracy): Accuracy()\n",
      "  (test_nlvr2_loss): Scalar()\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
